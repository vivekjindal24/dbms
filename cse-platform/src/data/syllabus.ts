export interface CLO {
  id: string;
  description: string;
}

export interface CO {
  id: string;
  description: string;
}

export interface Topic {
  id: string;
  title: string;
  subtopics?: string[];
  clos: string[];
  cos: string[];
  content: {
    introduction: string;
    concept: string;
    technicalDepth: string;
    examples: string;
    practical: string;
    exam: string;
    takeaways: string;
  };
}

export interface Unit {
  id: string;
  title: string;
  topics: Topic[];
}

export const clos: CLO[] = [
  { id: "CLO01", description: "Have a broad understanding of database concepts and database management system software" },
  { id: "CLO02", description: "Have a high-level understanding of major DBMS components and their function" },
  { id: "CLO03", description: "Be able to model an application's data requirements using conceptual modeling tools like ER diagrams and design database schemas based on the conceptual model." },
  { id: "CLO04", description: "Be able to write SQL commands to create tables and indexes, insert/update/delete data, and query data in a relational DBMS." },
  { id: "CLO05", description: "Be able to program a data-intensive application using DBMS APIs." },
];

export const cos: CO[] = [
  { id: "CO01", description: "Identify the basic concepts and various data model used in database design ER modelling concepts and architecture use and design queries using SQL" },
  { id: "CO02", description: "Apply relational database theory and be able to describe relational algebra expression, tuple and domain relation expression from queries." },
  { id: "CO03", description: "Recognize and identify the use of normalization and functional dependency. indexing and hashing technique used in database design." },
  { id: "CO04", description: "Recognize/ Identify the purpose of query processing and optimization and also demonstrate the basic of query evaluation." },
  { id: "CO05", description: "Apply and relate the concept of transaction, concurrency control and recovery in database." },
  { id: "CO06", description: "Understanding of recovery system and be familiar with introduction to web database, distribute databases, data warehousing and mining." },
];

export const units: Unit[] = [
  {
    id: "unit-1",
    title: "UNIT I: Introduction & Architecture",
    topics: [
      {
        id: "intro-to-db",
        title: "Introduction to Database",
        subtopics: ["Hierarchical, Network and Relational database"],
        clos: ["CLO01"],
        cos: ["CO01"],
        content: {
          introduction: "Database systems form the backbone of modern information management. Understanding the evolution from hierarchical and network models to the dominant relational paradigm provides essential context for appreciating current database technologies.",
          concept: "A database is an organized collection of data, generally stored and accessed electronically from a computer system. The Database Management System (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data.",
          technicalDepth: "Evolution of Databases: \n1. Hierarchical Model: Tree-like structure (1:N relationships). \n2. Network Model: Graph structure (M:N relationships). \n3. Relational Model: Table-based (Relations).",
          examples: "Example: Hierarchical (IMS), Network (CODASYL), Relational (MySQL, PostgreSQL, Oracle). Real-world: Banking uses relational databases for accounts and transactions.",
          practical: "Used in Banking systems, University records, Airline reservations.",
          exam: "Define DBMS. Compare Hierarchical, Network, and Relational models.",
          takeaways: "DBMS provides an interface to perform various operations like data creation, storing, and updating."
        }
      },
      {
        id: "db-architecture",
        title: "Database System Architecture",
        subtopics: ["Data abstraction", "Data independence", "DDL", "DML", "DCL"],
        clos: ["CLO02"],
        cos: ["CO01"],
        content: {
          introduction: "Database architecture defines the framework for how data is organized, stored, accessed, and managed. Understanding the layered architecture and data independence is crucial for designing scalable database systems.",
          concept: "Database architecture defines how data is viewed by different users. The Three-Schema Architecture separates the physical storage from the user view.",
          technicalDepth: "Three Levels of Abstraction:\n1. Physical Level: How data is stored.\n2. Logical Level: What data is stored.\n3. View Level: What part of data is seen by user.\n\nData Independence:\n- Logical Data Independence\n- Physical Data Independence",
          examples: "DDL Example: CREATE TABLE Students(ID INT, Name VARCHAR(100));\nDML Example: SELECT * FROM Students WHERE ID = 101;\nDCL Example: GRANT SELECT ON Students TO user_john;",
          practical: "Allows changing the database schema without affecting the application layer.",
          exam: "Explain the Three-Schema Architecture with a diagram. Differentiate between Physical and Logical Data Independence.",
          takeaways: "Abstraction hides complexity. Independence allows modification of one level without affecting others."
        }
      }
    ]
  },
  {
    id: "unit-2",
    title: "UNIT II: Data Models & Query Languages",
    topics: [
      {
        id: "data-models",
        title: "Data Models",
        subtopics: ["Entity-relationship model", "Network model", "Relational and Object oriented data Models", "Integrity constraints", "Data manipulation operations"],
        clos: ["CLO03"],
        cos: ["CO01"],
        content: {
          introduction: "Data models are abstract representations that define how data is structured, stored, and manipulated in a database. They serve as the blueprint for database design, helping us understand relationships between different data elements and enforcing rules that maintain data quality.",
          concept: "A data model is a conceptual representation of data objects, their associations, and the rules governing operations on them. The Entity-Relationship (ER) model is the most popular conceptual model for database design.\n\nKey Components:\n\n1. Entities: Real-world objects (Student, Course, Department)\n2. Attributes: Properties of entities (StudentID, Name, Age)\n3. Relationships: Associations between entities (Student ENROLLS Course)\n4. Cardinality: One-to-One (1:1), One-to-Many (1:N), Many-to-Many (M:N)\n\nThe ER model helps visualize database structure before implementation. It captures business requirements in a form that can be translated into relational tables.\n\nObject-Oriented Data Models extend this by supporting inheritance, encapsulation, and complex data types—useful for multimedia and CAD applications.",
          technicalDepth: "Entity-Relationship Model Components:\n\n1. Strong Entity: Has its own primary key (e.g., Student with StudentID)\n2. Weak Entity: Depends on strong entity (e.g., Dependent relies on Employee)\n3. Attributes Types:\n   - Simple vs Composite (Name vs {FirstName, LastName})\n   - Single-valued vs Multi-valued (Age vs PhoneNumbers)\n   - Stored vs Derived (DateOfBirth vs Age)\n\nRelationship Types:\n- Unary (recursive): Employee MANAGES Employee\n- Binary: Student ENROLLS Course\n- Ternary: Professor TEACHES Course IN Semester\n\nIntegrity Constraints:\n\n1. Domain Constraint: Values must be from specified domain\n   Example: Age must be integer between 0-120\n\n2. Entity Integrity: Primary key cannot be NULL\n   Why? Need unique identification for each tuple\n\n3. Referential Integrity: Foreign key must match primary key in referenced table\n   Example: StudentID in Enrollment must exist in Student table\n\n4. Key Constraints:\n   - Superkey: Any combination that uniquely identifies\n   - Candidate Key: Minimal superkey\n   - Primary Key: Chosen candidate key\n   - Alternate Key: Other candidate keys\n\nMapping ER to Relational:\n- Each entity becomes a table\n- Attributes become columns\n- Relationships handled via foreign keys or junction tables (for M:N)",
          examples: "ER Diagram Example - University Database:\n\nEntities:\nStudent(StudentID*, Name, Email, Major)\nCourse(CourseID*, CourseName, Credits)\nProfessor(ProfID*, ProfName, Department)\n\nRelationships:\n1. Student ENROLLS Course (M:N)\n   Junction Table: Enrollment(StudentID, CourseID, Grade, Semester)\n\n2. Professor TEACHES Course (1:N)\n   Foreign Key: Course.ProfID references Professor.ProfID\n\nConstraints in SQL:\n\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    Name VARCHAR(100) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    Age INT CHECK (Age >= 16 AND Age <= 100)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID INT,\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID),\n    FOREIGN KEY (StudentID) REFERENCES Student(StudentID)\n        ON DELETE CASCADE,\n    FOREIGN KEY (CourseID) REFERENCES Course(CourseID)\n        ON UPDATE CASCADE\n);\n\nReal-World Example - E-Commerce:\n- Customer HAS Address (1:N - composite attribute becomes table)\n- Order CONTAINS Product (M:N - junction table OrderItem)\n- Product BELONGS_TO Category (N:1 - foreign key in Product)",
          practical: "ER modeling is crucial in system design. Before writing any code, database architects create ER diagrams to:\n\n1. Communicate with stakeholders using visual representations\n2. Identify data requirements and relationships\n3. Prevent data redundancy and anomalies\n4. Plan for scalability and future changes\n\nReal applications:\n- E-commerce platforms model customers, orders, products\n- Hospital systems model patients, doctors, appointments, prescriptions\n- Banking systems model accounts, transactions, customers, loans\n\nTools: MySQL Workbench, Lucidchart, draw.io for ER diagram creation.",
          exam: "Important Questions:\n\n1. Draw ER diagram for library management system with entities: Book, Member, Author, Publisher\n2. Differentiate between strong and weak entities with examples\n3. Explain all types of integrity constraints with SQL examples\n4. Convert given ER diagram to relational schema\n5. What is the difference between candidate key and primary key?\n6. How do you handle M:N relationships in relational model?\n7. Define cardinality and participation constraints in relationships",
          takeaways: "Key Points:\n\n• Data models bridge the gap between real-world requirements and database implementation\n• ER model is the standard for conceptual database design\n• Integrity constraints ensure data accuracy and consistency\n• Proper modeling prevents data anomalies and redundancy\n• Understanding cardinality is crucial for correct table design\n• Foreign keys implement relationships in relational databases"
        }
      },
      {
        id: "relational-query-languages",
        title: "Relational Query Languages",
        subtopics: ["Relational algebra", "Tuple and domain relational calculus", "Open source and commercial DBMS"],
        clos: ["CLO04"],
        cos: ["CO02"],
        content: {
          introduction: "Query languages are formal languages used to retrieve and manipulate data in databases. They form the backbone of database interaction, allowing users to express complex data requirements in a structured way. Relational algebra provides the mathematical foundation for all query languages, including SQL.",
          concept: "Query languages come in two flavors:\n\n1. Procedural (Relational Algebra): You specify WHAT data you want and HOW to get it\n2. Non-procedural (Relational Calculus, SQL): You specify WHAT data you want, system figures out HOW\n\nRelational Algebra is a formal language with a set of operations for manipulating relations (tables). It's the theoretical basis for SQL and database query optimization.\n\nCore Principles:\n- Input: One or more relations\n- Output: A new relation\n- Operations can be composed (output of one is input to another)\n- Closure property: Result is also a relation\n\nWhy Learn Relational Algebra?\n- Understand query optimization\n- Foundation for SQL\n- Helps in query formulation\n- Used in query execution plans",
          technicalDepth: "Relational Algebra Operations:\n\n1. UNARY OPERATIONS (single relation):\n\nSelect (σ): Filters rows based on condition\n  Notation: σ<condition>(Relation)\n  Example: σ(Age>21)(Student) — students older than 21\n  Properties: σ(c1 AND c2)(R) = σ(c1)(σ(c2)(R)) — commutative\n\nProject (π): Selects specific columns\n  Notation: π<attributes>(Relation)\n  Example: π(Name, Major)(Student) — only names and majors\n  Note: Removes duplicates automatically\n\nRename (ρ): Changes relation or attribute names\n  Notation: ρ(NewName, Relation)\n  Useful for self-joins and clarity\n\n2. BINARY OPERATIONS (two relations):\n\nUnion (∪): Combines tuples from both relations\n  Requirement: Relations must be union-compatible (same schema)\n  R ∪ S: All tuples in R or S (no duplicates)\n\nIntersection (∩): Common tuples\n  R ∩ S: Tuples present in both R and S\n\nSet Difference (−): Tuples in first but not second\n  R − S: Tuples in R but not in S\n  Note: NOT commutative (R − S ≠ S − R)\n\nCartesian Product (×): All possible combinations\n  R × S: Each tuple in R paired with each in S\n  Result has |R| × |S| tuples\n  Columns: All columns from R and S\n\n3. JOIN OPERATIONS (most important):\n\nNatural Join (⋈): Combines relations based on common attributes\n  Employee ⋈ Department: Matches on common DeptID\n  Eliminates duplicate columns\n\nTheta Join (⋈θ): Join with condition\n  R ⋈(R.a=S.b) S\n  Generalized form of join\n\nEqui Join: Theta join with equality condition\n\nOuter Joins (preserve unmatched tuples):\n  - Left Outer Join (⟕): All tuples from left + matched from right\n  - Right Outer Join (⟖): All tuples from right + matched from left\n  - Full Outer Join (⟗): All tuples from both\n\nSemi Join (⋉): Only attributes from left relation\n  R ⋉ S = π(R attributes)(R ⋈ S)\n\n4. AGGREGATE OPERATIONS:\n\nGrouping (γ): Group by attributes and apply aggregate functions\n  γ(Dept, COUNT(EmpID))(Employee)\n  Functions: COUNT, SUM, AVG, MIN, MAX\n\nRelational Calculus:\n\nTuple Relational Calculus (TRC):\n  { t | P(t) } — set of tuples t for which predicate P is true\n  Example: { t | t ∈ Employee ∧ t.Salary > 50000 }\n\nDomain Relational Calculus (DRC):\n  Uses domain variables instead of tuple variables\n  Example: { <n, s> | ∃d (Employee(n, d, s) ∧ s > 50000) }\n\nSQL (Structured Query Language):\nBased on tuple relational calculus but with procedural elements.\nCommercial DBMS: MySQL, PostgreSQL, Oracle, SQL Server, MongoDB",
          examples: "Complex Query Examples:\n\nScenario: University Database\nStudent(SID, Name, Major, Age)\nEnrollment(SID, CID, Grade)\nCourse(CID, CourseName, Credits)\n\n1. Find names of students enrolled in 'DBMS'\n\nRelational Algebra:\nπ(Name)(σ(CourseName='DBMS')(Student ⋈ Enrollment ⋈ Course))\n\nSQL:\nSELECT DISTINCT S.Name\nFROM Student S\nJOIN Enrollment E ON S.SID = E.SID\nJOIN Course C ON E.CID = C.CID\nWHERE C.CourseName = 'DBMS';\n\n2. Students NOT enrolled in any course\n\nRelational Algebra:\nπ(SID)(Student) − π(SID)(Enrollment)\n\nSQL:\nSELECT SID FROM Student\nEXCEPT\nSELECT SID FROM Enrollment;\n\n3. Students enrolled in ALL courses\n\nRelational Algebra (Division):\nπ(SID, CID)(Enrollment) ÷ π(CID)(Course)\n\nSQL:\nSELECT S.SID\nFROM Student S\nWHERE NOT EXISTS (\n    SELECT C.CID FROM Course C\n    WHERE NOT EXISTS (\n        SELECT * FROM Enrollment E\n        WHERE E.SID = S.SID AND E.CID = C.CID\n    )\n);\n\n4. Average grade per student\n\nRelational Algebra:\nγ(SID, AVG(Grade))(Enrollment)\n\nSQL:\nSELECT SID, AVG(Grade) as AvgGrade\nFROM Enrollment\nGROUP BY SID;\n\n5. Self-Join: Find employees earning more than their manager\n\nSQL:\nSELECT E.Name\nFROM Employee E, Employee M\nWHERE E.ManagerID = M.EmpID\nAND E.Salary > M.Salary;\n\nOpen Source DBMS: MySQL, PostgreSQL, SQLite, MariaDB\nCommercial DBMS: Oracle, Microsoft SQL Server, IBM DB2",
          practical: "Query languages are used everywhere:\n\n1. Web Applications: Fetch user data, process transactions\n2. Analytics: Business intelligence, reporting dashboards\n3. Data Science: Extract data for machine learning models\n4. APIs: Backend services query databases for client requests\n\nPerformance Considerations:\n- Query optimization: DBMS converts SQL to efficient relational algebra\n- Indexing: Speeds up SELECT operations\n- Join order matters: Employee ⋈ Dept vs Dept ⋈ Employee\n- Understanding algebra helps write efficient queries\n\nReal-world SQL Usage:\n- Complex JOINs for related data (orders + customers + products)\n- Aggregations for reports (total sales per region)\n- Subqueries for filtering (students with above-average grades)\n- Transactions for consistency (bank transfers)",
          exam: "Expected Questions:\n\n1. Write relational algebra for: Find students enrolled in more than 3 courses\n2. Convert given SQL query to relational algebra expression\n3. Explain all types of JOIN operations with examples\n4. Differentiate between relational algebra and relational calculus\n5. What is division operation? When is it used?\n6. Write SQL query using GROUP BY and HAVING\n7. Explain query optimization with relational algebra\n8. Compare open-source vs commercial DBMS (features, licensing, performance)",
          takeaways: "Essential Concepts:\n\n• Relational algebra is the foundation of all query languages\n• SQL is based on relational calculus but includes procedural elements\n• Understanding algebra helps in query optimization\n• Join operations are the most powerful and commonly used\n• Set operations require union-compatible relations\n• Query execution plans use relational algebra internally\n• Choosing the right DBMS depends on requirements: scale, budget, features"
        }
      }
    ]
  },
  {
    id: "unit-3",
    title: "UNIT III: Relational Database Design",
    topics: [
      {
        id: "relational-design",
        title: "Relational Database Design",
        subtopics: ["Domain and data dependency", "Armstrong's axioms", "Functional dependencies", "Normal forms", "Dependency preservation", "Lossless design"],
        clos: ["CLO03"],
        cos: ["CO03"],
        content: {
          introduction: "Database design is the art and science of organizing data to minimize redundancy while ensuring integrity. Poor design leads to data anomalies—inconsistencies that creep in during updates, deletions, or insertions. Normalization is the systematic process that transforms poorly designed schemas into well-structured ones.",
          concept: "Normalization decomposes relations with anomalies into smaller, well-structured relations. The goal: eliminate redundancy without losing information.\n\nWhy Normalize?\n\n1. Update Anomaly: Changing data requires multiple updates\n   Example: If professor phone is stored with each course they teach, updating phone requires changing multiple rows\n\n2. Deletion Anomaly: Deleting one fact loses other facts\n   Example: Deleting last student in a course might delete course information\n\n3. Insertion Anomaly: Cannot insert some facts without others\n   Example: Cannot add new course without enrolling a student\n\nFunctional Dependencies:\nCore concept in normalization. X → Y means X determines Y.\n\nExample: StudentID → Name (StudentID determines Name)\nGiven StudentID, we can find exactly one Name.\n\nTypes:\n- Trivial FD: Y ⊆ X (Age, Name) → Age\n- Non-trivial: Y ⊈ X\n- Full FD: No proper subset of X determines Y\n- Partial FD: Some proper subset of X determines Y\n- Transitive FD: X → Y, Y → Z, then X → Z",
          technicalDepth: "Normal Forms (Progressive Refinement):\n\n0NF (Unnormalized): Repeating groups, nested structures\nExample: Student(ID, Name, Courses{CourseID, Grade})\n\n1NF (First Normal Form):\nRule: All attributes must contain atomic (indivisible) values\nNo repeating groups or arrays\n\nBefore: Student(ID, Name, Phones)\n  Phones = '123-4567, 987-6543'\nAfter: Student(ID, Name, Phone)\n  Two rows if two phones\n\n2NF (Second Normal Form):\nRule: 1NF + No partial dependencies\nApplies only when primary key is composite\n\nExample Problem:\nEnrollment(StudentID, CourseID, StudentName, Grade)\nPK: (StudentID, CourseID)\nIssue: StudentID → StudentName (partial dependency)\n\nSolution:\nStudent(StudentID, StudentName)\nEnrollment(StudentID, CourseID, Grade)\n\n3NF (Third Normal Form):\nRule: 2NF + No transitive dependencies\nNo non-key attribute depends on another non-key attribute\n\nExample Problem:\nEmployee(EmpID, EmpName, DeptID, DeptName)\nEmpID → DeptID, DeptID → DeptName (transitive)\n\nSolution:\nEmployee(EmpID, EmpName, DeptID)\nDepartment(DeptID, DeptName)\n\nBCNF (Boyce-Codd Normal Form):\nRule: For every non-trivial FD X → Y, X must be a superkey\nStricter than 3NF\n\nExample Problem:\nCourseSchedule(Student, Course, Instructor)\nAssumptions:\n- Each course has multiple sections\n- Each instructor teaches one section\n- Student can take only one section per course\n\nFDs: {Student, Course} → Instructor\n     Instructor → Course\nIssue: Instructor → Course violates BCNF (Instructor not superkey)\n\nSolution:\nInstructorCourse(Instructor, Course)\nStudentInstructor(Student, Instructor)\n\nArmstrong's Axioms (Inference Rules):\n\n1. Reflexivity: If Y ⊆ X, then X → Y\n2. Augmentation: If X → Y, then XZ → YZ\n3. Transitivity: If X → Y and Y → Z, then X → Z\n\nDerived Rules:\n4. Union: If X → Y and X → Z, then X → YZ\n5. Decomposition: If X → YZ, then X → Y and X → Z\n6. Pseudotransitivity: If X → Y and WY → Z, then WX → Z\n\nClosure of Attributes:\nFind all attributes determined by a set of attributes.\nUsed to find candidate keys.\n\nAlgorithm:\n1. Start with given attributes\n2. Apply FDs to add more attributes\n3. Repeat until no new attributes\n\nDecomposition Properties:\n\n1. Lossless Join: Can reconstruct original relation\n   Test: R1 ⋈ R2 = R\n\n2. Dependency Preservation: All FDs can be checked without joins\n   Important for efficiency",
          examples: "Step-by-Step Normalization:\n\nOriginal Relation (0NF):\nStudentCourse(StudentID, StudentName, Address, CourseID, \n              CourseName, InstructorID, InstructorName, Grade)\n\nSample Data:\n101, John, NYC, C1, DBMS, I1, Smith, A\n101, John, NYC, C2, OS, I2, Jones, B\n102, Mary, LA, C1, DBMS, I1, Smith, A\n\nProblems: Redundancy (John's info repeated), Update anomalies\n\nStep 1: 1NF (Already atomic)\n\nStep 2: 2NF\nIdentify FDs:\nStudentID → StudentName, Address\nCourseID → CourseName, InstructorID, InstructorName\n{StudentID, CourseID} → Grade\n\nDecompose:\nStudent(StudentID, StudentName, Address)\nCourse(CourseID, CourseName, InstructorID, InstructorName)\nEnrollment(StudentID, CourseID, Grade)\n\nStep 3: 3NF\nIn Course table: InstructorID → InstructorName (transitive)\n\nFinal Schema:\nStudent(StudentID, StudentName, Address)\nInstructor(InstructorID, InstructorName)\nCourse(CourseID, CourseName, InstructorID)\nEnrollment(StudentID, CourseID, Grade)\n\nSQL Implementation:\n\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    StudentName VARCHAR(100),\n    Address VARCHAR(200)\n);\n\nCREATE TABLE Instructor (\n    InstructorID INT PRIMARY KEY,\n    InstructorName VARCHAR(100)\n);\n\nCREATE TABLE Course (\n    CourseID VARCHAR(10) PRIMARY KEY,\n    CourseName VARCHAR(100),\n    InstructorID INT,\n    FOREIGN KEY (InstructorID) REFERENCES Instructor(InstructorID)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID VARCHAR(10),\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID),\n    FOREIGN KEY (StudentID) REFERENCES Student(StudentID),\n    FOREIGN KEY (CourseID) REFERENCES Course(CourseID)\n);",
          practical: "Real-World Considerations:\n\n1. Over-Normalization:\n   Too many tables can hurt performance (many joins)\n   Solution: Denormalize strategically for read-heavy systems\n\n2. When to Denormalize:\n   - Read performance critical (analytics, reporting)\n   - Acceptable redundancy (derived/cached data)\n   - NoSQL databases (MongoDB stores nested documents)\n\n3. Industry Practice:\n   - OLTP systems: Normalize to 3NF/BCNF\n   - Data warehouses: Denormalize (star schema, snowflake schema)\n   - E-commerce: Balance between normalized inventory and denormalized product displays\n\n4. Modern Approaches:\n   - Microservices: Each service has its own optimized schema\n   - Event sourcing: Store events, build projections\n   - CQRS: Separate read and write models",
          exam: "Important Exam Questions:\n\n1. Given relation R(A,B,C,D,E) with FDs: A→B, BC→E, ED→A\n   Find: All candidate keys, Highest normal form\n\n2. Normalize given unnormalized relation to BCNF\n\n3. Explain with examples: Update, Delete, Insert anomalies\n\n4. Difference between 3NF and BCNF with example where 3NF ≠ BCNF\n\n5. Prove Armstrong's Axioms are sound and complete\n\n6. Given FD set, find minimal cover (canonical cover)\n\n7. Check if decomposition is lossless and dependency-preserving\n\n8. When would you denormalize a database? Trade-offs?",
          takeaways: "Core Principles:\n\n• Normalization eliminates redundancy and prevents anomalies\n• Each normal form builds on previous ones (1NF → 2NF → 3NF → BCNF)\n• Functional dependencies are key to understanding normalization\n• BCNF is the ideal, but 3NF is often sufficient in practice\n• Decomposition should be lossless and preserve dependencies\n• Over-normalization can hurt read performance\n• Real systems balance normalization with performance needs"
        }
      },
      {
        id: "query-processing",
        title: "Query Processing and Optimization",
        subtopics: ["Evaluation of relational algebra expressions", "Query equivalence", "Join strategies", "Query optimization algorithms"],
        clos: ["CLO04"],
        cos: ["CO04"],
        content: {
          introduction: "When you write a SQL query, the database doesn't just blindly execute it. Behind the scenes, the query processor analyzes, transforms, and optimizes your query to find the fastest way to get results. Understanding this process helps you write better queries and diagnose performance issues.",
          concept: "Query processing is the set of activities performed by the DBMS to translate a high-level query (SQL) into an efficient execution plan.\n\nPhases of Query Processing:\n\n1. Parsing & Translation:\n   - Check syntax\n   - Verify table/column names exist\n   - Translate SQL to relational algebra\n   - Build query tree\n\n2. Optimization:\n   - Generate multiple execution plans\n   - Estimate cost of each plan\n   - Choose the best plan\n\n3. Evaluation:\n   - Execute the chosen plan\n   - Return results\n\nWhy Optimization Matters:\nConsider: SELECT * FROM Student JOIN Enrollment WHERE Grade='A'\n\nBad Plan: Join all students with all enrollments, then filter\nGood Plan: Filter first (σGrade='A'), then join smaller result\n\nFor large tables (millions of rows), the difference can be seconds vs hours.",
          technicalDepth: "Query Optimization Approaches:\n\n1. HEURISTIC OPTIMIZATION (Rule-Based):\n\nRules that generally improve performance:\n\nRule 1: Perform selections early\n  πname(σsalary>50000(Employee ⋈ Department))\n  Better: πname(σsalary>50000(Employee) ⋈ Department)\n  Why? Smaller intermediate result\n\nRule 2: Perform projections early\n  Reduce tuple size, fewer I/Os\n\nRule 3: Combine selections\n  σc1(σc2(R)) = σ(c1 AND c2)(R)\n  Single scan instead of two\n\nRule 4: Combine projections\n  Similar to selections\n\nRule 5: Break complex selections with OR\n  σ(c1 OR c2)(R) can use union of two index scans\n\nRule 6: Evaluate Cartesian products with joins\n  σcondition(R × S) = R ⋈condition S\n  Join algorithms are optimized\n\n2. COST-BASED OPTIMIZATION:\n\nEstimate cost of each plan using:\n\nA. Statistics:\n   - Number of tuples (n)\n   - Tuple size (bytes)\n   - Number of distinct values (V)\n   - Index structure (B-tree depth, hash buckets)\n\nB. Cost Components:\n   - Disk I/O cost (dominant factor)\n   - CPU cost (comparisons, sorting)\n   - Memory usage\n   - Network cost (distributed databases)\n\nC. Selectivity Estimation:\n   Selectivity = fraction of tuples satisfying condition\n\n   For A = value: 1/V(A) (uniform distribution assumption)\n   For A > value: (max - value)/(max - min)\n   For A AND B: s(A) × s(B) (independence assumption)\n   For A OR B: s(A) + s(B) - s(A)×s(B)\n\nJOIN STRATEGIES:\n\n1. Nested Loop Join:\n   For each tuple r in R:\n     For each tuple s in S:\n       If join-condition(r,s): output\n   Cost: n(R) + n(R)×n(S)\n   Best when: One relation is small\n\n2. Block Nested Loop:\n   Process blocks instead of tuples\n   Cost: b(R) + b(R)×b(S)\n   Better I/O performance\n\n3. Index Nested Loop:\n   For each tuple r in R:\n     Use index on S to find matching tuples\n   Cost: n(R) + n(R)×c\n   Where c = cost of index lookup\n   Best when: Index exists on join attribute\n\n4. Sort-Merge Join:\n   Sort both relations on join attribute\n   Merge sorted relations\n   Cost: b(R) + b(S) + sort cost\n   Best for: Large sorted or nearly-sorted relations\n\n5. Hash Join:\n   Build phase: Hash smaller relation\n   Probe phase: Hash larger relation, probe hash table\n   Cost: 3(b(R) + b(S))\n   Best for: Equi-joins on large unsorted relations\n   Most commonly used in practice\n\nQuery Equivalence:\nTwo queries are equivalent if they produce same result for any database state.\n\nEquivalence Rules:\n1. σ(c1 AND c2)(R) = σc1(σc2(R))\n2. σc1(σc2(R)) = σc2(σc1(R))\n3. πL1(πL2(...(πLn(R)))) = πL1(R)\n4. σc(R × S) = R ⋈c S\n5. R ⋈ S = S ⋈ R (commutative)\n6. (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T) (associative)\n\nDynamic Programming for Join Ordering:\nFor n tables, there are (2n-2)!/(n-1)! possible join orders.\nDP finds optimal order in O(n· 2^n) time.",
          examples: "Example 1: Query Transformation\n\nOriginal SQL:\nSELECT name\nFROM Employee, Department\nWHERE Employee.deptId = Department.id\n  AND Department.location = 'NYC'\n  AND Employee.salary > 50000;\n\nNaive Plan:\n1. Employee × Department (huge Cartesian product)\n2. Filter all conditions\n3. Project name\n\nOptimized Plan:\n1. σlocation='NYC'(Department) — filter departments first\n2. σsalary>50000(Employee) — filter employees first\n3. Filtered_Employee ⋈ Filtered_Department — smaller join\n4. πname — project at end\n\nExample 2: Join Strategy Selection\n\nQuery: Employee ⋈deptId Department\n\nScenario A: Employee (1M rows), Department (50 rows)\nBest: Index Nested Loop\n  - Build index on Employee.deptId\n  - For each department, lookup matching employees\n  - Cost: 50 index lookups\n\nScenario B: Employee (1M rows), Department (500K rows)\nBest: Hash Join\n  - Hash smaller relation (Department)\n  - Probe with Employee\n  - Single pass through both relations\n\nExample 3: Cost Estimation\n\nGiven:\n- Employee: 10,000 tuples, 100 tuples/block = 100 blocks\n- Department: 50 tuples, 5 tuples/block = 10 blocks\n- Join: Employee.deptId = Department.id\n\nNested Loop:\n  Cost = 100 + 10,000 × 10 = 100,100 block accesses\n\nBlock Nested Loop:\n  Cost = 100 + 100 × 10 = 1,100 block accesses\n\nHash Join:\n  Cost = 3(100 + 10) = 330 block accesses\n\nWinner: Hash Join\n\nEXPLAIN Command:\n\nEXPLAIN SELECT * FROM Employee WHERE salary > 50000;\n\nOutput shows:\n- Scan type (Sequential, Index, Bitmap)\n- Rows estimate\n- Cost estimate\n- Filter conditions\n- Join method\n\nExample Output:\nSeq Scan on employee (cost=0.00..180.00 rows=1000 width=50)\n  Filter: (salary > 50000)\n\nVs with index:\nIndex Scan using idx_salary (cost=0.00..45.00 rows=1000 width=50)\n  Index Cond: (salary > 50000)",
          practical: "Performance Tuning Tips:\n\n1. Write Efficient Queries:\n   - Use WHERE to filter early\n   - Avoid SELECT *, specify needed columns\n   - Use LIMIT when possible\n   - Avoid functions on indexed columns\n\n2. Indexing Strategy:\n   - Index foreign keys\n   - Index columns in WHERE, JOIN, ORDER BY\n   - Avoid over-indexing (slows INSERT/UPDATE)\n   - Consider composite indexes for multiple columns\n\n3. Understand Query Plans:\n   - Use EXPLAIN to see execution plan\n   - Look for sequential scans on large tables\n   - Check join methods\n   - Monitor rows estimates vs actual\n\n4. Database Configuration:\n   - Buffer pool size (memory for caching)\n   - Work_mem (memory for sorting, hashing)\n   - Statistics collection (ANALYZE command)\n\n5. Query Hints (use sparingly):\n   - Force index usage\n   - Specify join order\n   - Enable/disable certain optimizations\n\nReal-World Scenarios:\n- E-commerce: Optimize product search queries\n- Analytics: Optimize complex aggregation queries\n- Social media: Optimize feed generation queries\n- Financial: Optimize transaction history queries",
          exam: "Expected Questions:\n\n1. Explain phases of query processing with diagram\n2. What is the difference between heuristic and cost-based optimization?\n3. Compare all join strategies: nested loop, block nested loop, index nested loop, sort-merge, hash\n4. Given query and statistics, calculate cost for different join orders\n5. Transform given SQL query to optimized relational algebra\n6. Explain dynamic programming approach for join ordering\n7. What is selectivity? How is it estimated?\n8. Given EXPLAIN output, identify performance bottlenecks\n9. Why is pushing selections down beneficial?\n10. Explain the role of indexes in query optimization",
          takeaways: "Key Concepts:\n\n• Query optimization chooses the fastest execution plan from many alternatives\n• Heuristic rules provide quick optimizations (push selections down)\n• Cost-based optimization uses statistics to estimate plan costs\n• Join strategy selection depends on table sizes and available indexes\n• Hash join is usually best for large equi-joins\n• Index nested loop is best when one table is small\n• Understanding optimization helps write efficient queries\n• EXPLAIN command reveals execution plans for tuning"
        }
      }
    ]
  },
  {
    id: "unit-4",
    title: "UNIT IV: Storage & Transactions",
    topics: [
      {
        id: "storage-strategies",
        title: "Storage Strategies",
        subtopics: ["Indices", "B-trees", "Hashing"],
        clos: ["CLO02"],
        cos: ["CO03"],
        content: {
          introduction: "The gap between memory speed and disk speed is vast—memory access takes nanoseconds, disk access takes milliseconds. That's a million-fold difference. Storage strategies bridge this gap through clever data structures, caching, and organization techniques that minimize disk I/O, the primary bottleneck in database performance.",
          concept: "Storage strategies address the physical organization of data on disk and in memory to optimize access patterns.\n\nStorage Hierarchy:\n\n1. Primary Storage (RAM):\n   - Fast (nanoseconds)\n   - Volatile (lost on power off)\n   - Expensive\n   - Limited capacity\n\n2. Secondary Storage (HDD/SSD):\n   - Slower (milliseconds for HDD, microseconds for SSD)\n   - Persistent\n   - Cheaper\n   - Large capacity\n\n3. Tertiary Storage (Tape, Cloud):\n   - Slowest (seconds)\n   - Archival purposes\n\nKey Principle: Minimize disk I/O by:\n- Keeping frequently accessed data in memory (buffer pool)\n- Using indexes to avoid full table scans\n- Storing related data together (clustering)\n- Compressing data to reduce I/O volume",
          technicalDepth: "FILE ORGANIZATION:\n\n1. Heap File (Unordered):\n   Records stored in insertion order\n   Insert: O(1) - append at end\n   Search: O(n) - scan entire file\n   Delete: O(n) - find then mark deleted\n   Best for: Bulk loading, full table scans\n\n2. Sorted File:\n   Records sorted by search key\n   Insert: O(n) - maintain order\n   Search: O(log n) - binary search\n   Range queries: Efficient\n   Best for: Static data with range queries\n\n3. Hash File:\n   Records distributed across buckets using hash function\n   Search: O(1) average - compute hash, go to bucket\n   Range queries: Inefficient (requires full scan)\n   Best for: Exact match lookups\n\nINDEXING STRUCTURES:\n\n1. B-TREE:\n\nProperties:\n- Balanced tree (all leaves at same level)\n- Order d: Each node has [d, 2d] keys (except root)\n- Internal nodes: [d+1, 2d+1] children\n- Keeps data sorted\n\nStructure:\nNode: [P1, K1, P2, K2, ..., Pn, Kn, Pn+1]\n- Ki = keys (sorted)\n- Pi = pointers to children or data\n- All keys in subtree Pi < Ki < all keys in subtree Pi+1\n\nSearch Algorithm:\n1. Start at root\n2. Binary search keys in node\n3. Follow appropriate pointer\n4. Repeat until leaf\n\nInsertion:\n1. Search to find leaf\n2. Insert key in sorted order\n3. If leaf overflows (> 2d keys):\n   - Split into two nodes\n   - Promote middle key to parent\n   - Recursively split parent if needed\n\nDeletion:\n1. Search and remove key\n2. If underflow (< d keys):\n   - Borrow from sibling, OR\n   - Merge with sibling\n   - Recursively fix parent\n\nCost Analysis (n records, order d):\n- Tree height: log_d(n)\n- Search: O(log n) disk I/Os\n- Insert/Delete: O(log n) disk I/Os\n\n2. B+ TREE (Most Common in Databases):\n\nDifferences from B-Tree:\n1. All data in leaf nodes\n2. Internal nodes only store keys + pointers\n3. Leaf nodes linked (sequential access)\n\nAdvantages:\n- Higher fanout (more keys per internal node)\n- Shorter tree (fewer I/Os)\n- Efficient range scans (follow leaf pointers)\n- Sequential access without tree traversal\n\nExample B+ Tree (order d=2):\n                [30]\n               /    \\\n          [10,20]  [40,50]\n         /  |  \\    /  |  \\\nLeaves: [5,7][12,15][25,28][35,38][45,48][55,60]\nLeaves linked: → allows sequential scan\n\n3. HASH INDEX:\n\nStatic Hashing:\nhash(key) % M = bucket number\nIssue: Fixed M, poor with growing data\n\nDynamic Hashing (Extendible Hashing):\n- Bucket directory grows/shrinks\n- Only affected buckets split\n- Efficient for insertions\n\nLinear Hashing:\n- No directory\n- Gradual bucket splitting\n- Handles overflows with chaining\n\nBUFFER MANAGEMENT:\n\nBuffer Pool: Area of memory for caching disk pages\n\nReplacement Policies:\n\n1. LRU (Least Recently Used):\n   Replace page not used for longest time\n   Good general-purpose policy\n\n2. MRU (Most Recently Used):\n   Replace most recently used\n   Good for sequential scans\n\n3. Clock Algorithm:\n   Approximates LRU with less overhead\n   Each page has reference bit\n\n4. LRU-K:\n   Track K most recent accesses\n   Better for database workloads\n\nPin Count: Number of active users of a page\n- Pinned pages cannot be replaced\n- Must unpin after use\n\nDirty Bit: Indicates modified page\n- Must write back before replacement\n- WAL rule: Log before data\n\nCLUSTERING:\n\nStore related records physically close\n\nTypes:\n1. Intra-file clustering: Related records in same file\n2. Inter-file clustering: Records from multiple tables together\n\nClustered Index:\n- Data stored in index order\n- Only one per table\n- Improves range queries\n- Slows insertions (maintain order)\n\nExample: Customer table clustered by CustomerID\n- CustomerID 1-1000 in block 1\n- CustomerID 1001-2000 in block 2\n- Range query efficient",
          examples: "Example 1: B+ Tree Operations\n\nInitial B+ Tree (order 3, max 5 keys per node):\n                     [30]\n                   /      \\\n             [10,20]      [40,50]\n            /   |   \\      /  |  \\\nLeaves: [5,7,9][12,15,18][25,27,29][35,38][45,48][55,60]\n\nInsert 13:\n1. Find correct leaf: [12,15,18]\n2. Insert: [12,13,15,18]\n3. No overflow, done\n\nInsert 14:\n1. Find leaf: [12,13,15,18]\n2. Insert: [12,13,14,15,18]\n3. No overflow, done\n\nInsert 16:\n1. Find leaf: [12,13,14,15,18]\n2. Insert: [12,13,14,15,16,18]\n3. Overflow! Split:\n   Left: [12,13,14]\n   Right: [15,16,18]\n   Promote 15 to parent: [10,15,20]\n\nExample 2: Index Selection\n\nQuery: SELECT * FROM Orders WHERE OrderDate BETWEEN '2024-01-01' AND '2024-01-31';\n\nOption A: No index\n- Full table scan: 1M orders × 100 bytes = 100MB\n- Assume 4KB pages: 25,000 page reads\n\nOption B: B+ Tree index on OrderDate\n- Assume 5% orders in range = 50K orders\n- Tree height: 3 levels\n- Cost: 3 (tree traversal) + 50K/100 (data pages) = 503 page reads\n- 50x faster!\n\nExample 3: Hash vs B+ Tree\n\nExact Match Query:\nSELECT * FROM Employee WHERE EmployeeID = 12345;\n\nHash Index:\n- hash(12345) % 100 = 45\n- Go directly to bucket 45\n- Cost: 1-2 I/Os (bucket + maybe overflow)\n\nB+ Tree Index:\n- Traverse tree: log_d(n) = 3 levels\n- Cost: 3 I/Os\n\nWinner: Hash (for exact match)\n\nRange Query:\nSELECT * FROM Employee WHERE EmployeeID BETWEEN 10000 AND 20000;\n\nHash Index:\n- No ordering information\n- Must scan all buckets\n- Cost: All buckets\n\nB+ Tree Index:\n- Find 10000, then sequential scan\n- Cost: log_d(n) + result size\n\nWinner: B+ Tree (for range)\n\nSQL Index Creation:\n\nCREATE INDEX idx_employee_dept ON Employee(DepartmentID);\nCREATE UNIQUE INDEX idx_employee_email ON Employee(Email);\nCREATE INDEX idx_order_date ON Orders(OrderDate);\n\n-- Composite index\nCREATE INDEX idx_employee_name ON Employee(LastName, FirstName);\n\n-- Covering index (includes non-key columns)\nCREATE INDEX idx_employee_salary ON Employee(DepartmentID) INCLUDE (Salary);\n\n-- Partial index (PostgreSQL)\nCREATE INDEX idx_active_orders ON Orders(OrderDate) WHERE Status='Active';",
          practical: "Best Practices:\n\n1. Index Strategy:\n   - Index foreign keys (join performance)\n   - Index WHERE clause columns\n   - Index ORDER BY columns\n   - Don't over-index (slows writes)\n   - Monitor index usage (drop unused)\n\n2. When to Use Each Index Type:\n   - B+ Tree: Default choice, handles all queries\n   - Hash: Exact match on high-cardinality columns\n   - Bitmap: Low-cardinality columns (gender, status)\n   - Full-text: Text search\n   - Spatial: Geographic data\n\n3. Clustering Decisions:\n   - Cluster on most frequent range query column\n   - Consider write vs read trade-off\n   - Only one clustered index per table\n\n4. Buffer Pool Tuning:\n   - PostgreSQL: shared_buffers (25% of RAM)\n   - MySQL: innodb_buffer_pool_size (70-80% of RAM)\n   - Monitor cache hit ratio (aim for >99%)\n\n5. Storage Hardware Considerations:\n   - SSD: Lower latency, random I/O less painful\n   - HDD: Optimize for sequential access\n   - Consider NVMe for extremely high throughput\n\nReal-World Scenarios:\n- E-commerce: Index product searches, cluster orders by date\n- Social media: Index user queries, hash indexes for user IDs\n- Analytics: Columnar storage, heavy compression\n- Banking: Cluster transactions by account, index timestamps",
          exam: "Important Questions:\n\n1. Compare heap, sorted, and hash file organizations. When to use each?\n2. Explain B+ Tree structure. Why preferred over B-Tree?\n3. Show step-by-step insertion/deletion in B+ Tree\n4. Calculate cost of query with and without index\n5. Explain buffer replacement policies: LRU, MRU, Clock\n6. What is a clustered index? Pros and cons?\n7. Compare hash index vs B+ Tree index\n8. Explain extendible hashing. How does it handle growth?\n9. What factors determine index selectivity?\n10. Design indexing strategy for given schema and query workload",
          takeaways: "Key Principles:\n\n• Disk I/O is the primary bottleneck; minimize it with indexes and caching\n• B+ Tree is the default index: handles all query types, keeps data sorted\n• Hash indexes excel at exact matches but can't do ranges\n• Clustered indexes physically order data, improving range queries\n• Buffer pool caches hot pages, dramatically improving performance\n• Over-indexing hurts write performance; balance reads vs writes\n• Modern SSDs change trade-offs but don't eliminate I/O concerns"
        }
      },
      {
        id: "transaction-processing",
        title: "Transaction Processing",
        subtopics: ["Concurrency control", "ACID property", "Serializability", "Schedules", "Locking and timestamp based schedules", "Multi-version and optimistic concurrency control schemes", "Database recovery"],
        clos: ["CLO05"],
        cos: ["CO05"],
        content: {
          introduction: "Imagine transferring $500 from your savings to checking account. The bank's system must: (1) deduct $500 from savings, (2) add $500 to checking. What if the system crashes between steps? You'd lose $500! Transactions ensure that either both steps happen, or neither does—no in-between states.",
          concept: "A transaction is a logical unit of work that must be executed completely or not at all. Transactions guarantee database consistency even when facing failures (crashes, power outages) and concurrent access (multiple users).\n\nACID Properties:\n\nA - Atomicity: All or nothing\n  Either all operations succeed (commit) or none (abort/rollback)\n  No partial transactions\n\nC - Consistency: Valid state to valid state\n  Database constraints always satisfied\n  Business rules preserved\n\nI - Isolation: Concurrent transactions don't interfere\n  Each transaction sees consistent snapshot\n  Result same as if transactions ran serially\n\nD - Durability: Once committed, changes persist\n  Survives crashes, power failures\n  Written to stable storage\n\nTransaction States:\n\n1. Active: Initial state, executing\n2. Partially Committed: After final statement, before commit\n3. Committed: Successfully completed\n4. Failed: Cannot proceed (error, abort)\n5. Aborted: Rolled back, database restored\n\nAfter abort:\n- Restart transaction, OR\n- Kill transaction",
          technicalDepth: "CONCURRENCY CONTROL:\n\nProblem: Concurrent transactions can cause anomalies\n\n1. Lost Update:\n   T1: Read X=100\n   T2: Read X=100\n   T1: Write X=150 (added 50)\n   T2: Write X=120 (added 20)\n   Final: X=120 (T1's update lost!)\n\n2. Dirty Read (Reading Uncommitted Data):\n   T1: Write X=200\n   T2: Read X=200\n   T1: Rollback (X back to 100)\n   T2 saw data that never existed!\n\n3. Non-Repeatable Read:\n   T1: Read X=100\n   T2: Write X=200, Commit\n   T1: Read X=200\n   T1 sees different value in same transaction!\n\n4. Phantom Read:\n   T1: SELECT COUNT(*) WHERE age>20 (returns 10)\n   T2: INSERT person with age=25, Commit\n   T1: SELECT COUNT(*) WHERE age>20 (returns 11)\n   New row appeared!\n\nCONCURRENCY CONTROL PROTOCOLS:\n\n1. LOCK-BASED PROTOCOLS:\n\nLock Types:\n- Shared Lock (S): Read access, multiple allowed\n- Exclusive Lock (X): Write access, exclusive\n\nCompatibility Matrix:\n       S    X\n   S   Y    N\n   X   N    N\n\nTwo-Phase Locking (2PL):\n\nPhase 1 (Growing): Acquire locks, no releases\nPhase 2 (Shrinking): Release locks, no acquisitions\n\nGuarantees: Serializability (equivalent to some serial order)\n\nProblem: Can deadlock\n\nVariants:\n\na) Strict 2PL:\n   Hold all locks until commit/abort\n   Prevents dirty reads\n   Most commonly used\n\nb) Rigorous 2PL:\n   Hold all locks until commit/abort\n   AND release all at once\n\nDeadlock Example:\n   T1: Lock A\n   T2: Lock B\n   T1: Wait for B (held by T2)\n   T2: Wait for A (held by T1)\n   Deadlock!\n\nDeadlock Handling:\n\n1. Detection:\n   - Build wait-for graph\n   - Detect cycles\n   - Abort victim transaction\n\n2. Prevention:\n   - Wait-Die: Older waits, younger aborts\n   - Wound-Wait: Older preempts, younger waits\n   - Timeout: Abort if waiting too long\n\n3. Avoidance:\n   - Order resources\n   - All transactions acquire locks in same order\n\n2. TIMESTAMP-BASED PROTOCOLS:\n\nEach transaction assigned unique timestamp (start time)\nOrder transactions by timestamp\n\nRules:\n- TS(Ti) < TS(Tj) ⇒ Ti must finish before Tj starts\n\nFor each data item X, track:\n- W-timestamp(X): Last write timestamp\n- R-timestamp(X): Last read timestamp\n\nOn Ti reads X:\n  If TS(Ti) < W-timestamp(X): Abort Ti (too old)\n  Else: Execute, update R-timestamp(X)\n\nOn Ti writes X:\n  If TS(Ti) < R-timestamp(X): Abort Ti (too late)\n  If TS(Ti) < W-timestamp(X): Abort Ti (obsolete)\n  Else: Execute, update W-timestamp(X)\n\nAdvantage: No deadlocks\nDisadvantage: More aborts\n\n3. MULTIVERSION CONCURRENCY CONTROL (MVCC):\n\nIdea: Keep multiple versions of each data item\nReaders don't block writers, writers don't block readers\n\nEach write creates new version with timestamp\nReads see appropriate version based on transaction timestamp\n\nUsed by: PostgreSQL, Oracle, SQL Server\n\nExample:\n   T1 (TS=1): Read X (sees version X1)\n   T2 (TS=2): Write X (creates version X2)\n   T1: Read X (still sees X1, consistent snapshot)\n   T3 (TS=3): Read X (sees X2)\n\nRECOVERY:\n\nTypes of Failures:\n\n1. Transaction Failure:\n   - Logical error (division by zero)\n   - System error (deadlock)\n   Solution: Rollback transaction\n\n2. System Crash:\n   - Power failure, OS crash\n   - Memory contents lost\n   Solution: Recovery from log\n\n3. Disk Failure:\n   - Physical damage to disk\n   Solution: Restore from backup\n\nRecovery Techniques:\n\n1. LOG-BASED RECOVERY:\n\nWrite-Ahead Logging (WAL):\n   Rule: Log record must be written to disk BEFORE data\n\nLog Records:\n   <Ti start>\n   <Ti, X, old_value, new_value>\n   <Ti commit>\n   <Ti abort>\n\nRecovery Algorithm (ARIES):\n\nPhase 1: Analysis\n   - Determine which transactions to undo/redo\n   - Identify dirty pages\n\nPhase 2: Redo\n   - Repeat history (all changes, even aborted)\n   - Restore database to crash state\n\nPhase 3: Undo\n   - Rollback incomplete transactions\n   - Use log to reverse changes\n\n2. CHECKPOINTING:\n\nProblem: Log grows forever\nSolution: Periodic checkpoints\n\nCheckpoint Process:\n   1. Stop accepting new transactions\n   2. Write all dirty buffers to disk\n   3. Write <checkpoint> to log\n   4. Resume transactions\n\nRecovery:\n   - Start from last checkpoint (not beginning)\n   - Reduces recovery time\n\nISOLATION LEVELS (SQL Standard):\n\n1. Read Uncommitted:\n   - No isolation\n   - Allows: Dirty read, non-repeatable read, phantom\n   - Fastest, least safe\n\n2. Read Committed:\n   - Prevents dirty reads\n   - Allows: Non-repeatable read, phantom\n   - Default in many systems\n\n3. Repeatable Read:\n   - Prevents dirty reads, non-repeatable reads\n   - Allows: Phantom\n\n4. Serializable:\n   - Full isolation\n   - Prevents all anomalies\n   - Slowest, safest\n\nSQL Syntax:\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;",
          examples: "Example 1: Transaction in SQL\n\n-- Bank Transfer: Savings to Checking\nBEGIN TRANSACTION;\n\n-- Step 1: Deduct from savings\nUPDATE Account\nSET Balance = Balance - 500\nWHERE AccountID = 101 AND AccountType = 'Savings';\n\n-- Step 2: Add to checking\nUPDATE Account\nSET Balance = Balance + 500\nWHERE AccountID = 101 AND AccountType = 'Checking';\n\n-- Verify balance constraints\nIF (SELECT Balance FROM Account WHERE AccountID=101 AND AccountType='Savings') < 0\n  ROLLBACK;\nELSE\n  COMMIT;\nEND TRANSACTION;\n\nExample 2: Deadlock Scenario\n\nTime | Transaction T1              | Transaction T2\n-----|----------------------------|---------------------------\nt1   | BEGIN                      |\nt2   |                            | BEGIN\nt3   | LOCK A (Exclusive)         |\nt4   |                            | LOCK B (Exclusive)\nt5   | UPDATE A SET ...           |\nt6   |                            | UPDATE B SET ...\nt7   | LOCK B (Wait for T2)       |\nt8   |                            | LOCK A (Wait for T1)\nt9   | DEADLOCK DETECTED          |\nt10  | T2 ABORTED (Victim)        |\nt11  | LOCK B (Acquired)          |\nt12  | COMMIT                     |\n\nExample 3: Recovery Scenario\n\nLog Contents:\n<T1 start>\n<T1, X, 100, 150>\n<T1, Y, 200, 250>\n<T1 commit>\n<T2 start>\n<T2, X, 150, 200>\n<T2, Y, 250, 300>\n--- CRASH ---\n\nRecovery:\n1. Redo T1 (committed): X=150, Y=250\n2. Undo T2 (not committed): X=150 (revert T2's change), Y=250\n\nFinal State: X=150, Y=250\n\nExample 4: Isolation Levels in Action\n\n-- Session 1\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates Balance to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Reads 500 (DIRTY READ!)\nCOMMIT;\n\n-- With READ COMMITTED\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Still reads 1000\nCOMMIT;\n\n-- With SERIALIZABLE (PostgreSQL)\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRANSACTION;\nSELECT SUM(Balance) FROM Account; -- Reads sum\n-- (Session 2 inserts new account)\nSELECT SUM(Balance) FROM Account; -- Same sum (no phantom)\nCOMMIT;",
          practical: "Best Practices:\n\n1. Transaction Design:\n   - Keep transactions short\n   - Acquire locks in consistent order (prevent deadlock)\n   - Avoid user interaction within transaction\n   - Handle exceptions (rollback on error)\n\n2. Choosing Isolation Level:\n   - READ UNCOMMITTED: Never (except analytics)\n   - READ COMMITTED: Default for most applications\n   - REPEATABLE READ: Financial transactions, reports\n   - SERIALIZABLE: Critical data, when correctness > performance\n\n3. Performance Tuning:\n   - Monitor lock wait times\n   - Identify long-running transactions\n   - Use row-level locking (not table-level)\n   - Consider optimistic locking for low contention\n\n4. Deadlock Prevention:\n   - Access tables in same order\n   - Use timeouts\n   - Keep transactions short\n   - Retry aborted transactions\n\n5. Application-Level Considerations:\n   - Implement retry logic\n   - Use connection pooling\n   - Consider eventual consistency (NoSQL)\n   - Saga pattern for distributed transactions\n\nReal-World Usage:\n- Banking: SERIALIZABLE for transfers\n- E-commerce: READ COMMITTED for order processing\n- Social media: READ COMMITTED, eventual consistency for likes\n- Stock trading: SERIALIZABLE with strict locking\n- Analytics: READ UNCOMMITTED (stale data acceptable)",
          exam: "Key Questions:\n\n1. Explain ACID properties with examples\n2. Draw transaction state diagram\n3. What problems arise from concurrent transactions? Give examples.\n4. Explain Two-Phase Locking protocol. How does it ensure serializability?\n5. Compare timestamp-based vs lock-based concurrency control\n6. What is a deadlock? Explain detection and prevention strategies.\n7. Explain Write-Ahead Logging and its importance in recovery\n8. Describe ARIES recovery algorithm (Analysis, Redo, Undo)\n9. Compare SQL isolation levels. Which allows which anomalies?\n10. Given a schedule, determine if it's serializable\n11. Show recovery steps given transaction log and crash point\n12. Explain MVCC. Why do readers not block writers?",
          takeaways: "Core Concepts:\n\n• ACID properties guarantee reliable transactions\n• Atomicity: All or nothing (enforced by logging and recovery)\n• Consistency: Maintain constraints (enforced by application + DBMS)\n• Isolation: Concurrent transactions don't interfere (concurrency control)\n• Durability: Committed changes survive crashes (write-ahead logging)\n• Two-Phase Locking ensures serializability but can deadlock\n• MVCC allows high concurrency by keeping multiple versions\n• Recovery uses logs to redo committed and undo uncommitted transactions\n• Isolation levels balance consistency with performance"
        }
      }
    ]
  },
  {
    id: "unit-5",
    title: "UNIT V: Security & Advanced Topics",
    topics: [
      {
        id: "database-security",
        title: "Database Security",
        subtopics: ["Authentication", "Authorization and access control", "DAC", "MAC and RBAC models", "Intrusion detection SQL injection"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "In 2023, data breaches exposed over 3 billion records globally. Equifax breach, Capital One breach, Marriott breach—the list goes on. Database security isn't optional; it's essential. A single SQL injection or weak access control can expose millions of customer records, leading to financial losses, legal consequences, and destroyed trust.",
          concept: "Database security protects against unauthorized access, data breaches, and malicious attacks through multiple layers of defense.\n\nSecurity Objectives (CIA Triad):\n\n1. Confidentiality: Only authorized users access data\n   Prevent: Unauthorized disclosure, eavesdropping\n\n2. Integrity: Data accuracy and consistency\n   Prevent: Unauthorized modification, corruption\n\n3. Availability: Authorized users can access when needed\n   Prevent: DoS attacks, system failures\n\nSecurity Layers:\n\n1. Network Security: Firewalls, VPNs, encryption\n2. OS Security: User accounts, file permissions\n3. Database Security: Authentication, authorization, auditing\n4. Application Security: Input validation, secure coding\n\nKey Threats:\n- SQL Injection: Malicious SQL in user input\n- Unauthorized Access: Weak passwords, stolen credentials\n- Privilege Escalation: Gaining higher access than authorized\n- Insider Threats: Malicious employees\n- Data Leakage: Unencrypted data transmission\n- DoS/DDoS: Overwhelming system with requests",
          technicalDepth: "AUTHENTICATION:\n\nVerifying user identity\n\nMethods:\n\n1. Password-Based:\n   - Most common\n   - Store hashed passwords (SHA-256, bcrypt)\n   - Never store plaintext\n   - Salting prevents rainbow table attacks\n\n2. Multi-Factor Authentication (MFA):\n   - Something you know (password)\n   - Something you have (token, phone)\n   - Something you are (biometric)\n\n3. Certificate-Based:\n   - Digital certificates (X.509)\n   - Public key infrastructure (PKI)\n\n4. OAuth/SAML:\n   - Federated authentication\n   - Single Sign-On (SSO)\n\nAUTHORIZATION & ACCESS CONTROL:\n\nDetermining what authenticated users can do\n\n1. DISCRETIONARY ACCESS CONTROL (DAC):\n\nOwner controls access to resources\n\nSQL Commands:\nGRANT SELECT, INSERT ON Employees TO user1;\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\nREVOKE DELETE ON Orders FROM user2;\n\nGRANT Options:\n- SELECT: Read data\n- INSERT: Add new data\n- UPDATE: Modify existing data\n- DELETE: Remove data\n- EXECUTE: Run stored procedures\n\nWITH GRANT OPTION: User can grant permissions to others\nGRANT SELECT ON Employees TO user1 WITH GRANT OPTION;\n\nProblems with DAC:\n- Permissions can spread uncontrollably\n- No central policy enforcement\n- Vulnerable to Trojan horse attacks\n\n2. MANDATORY ACCESS CONTROL (MAC):\n\nSystem enforces access policy based on security labels\n\nConcepts:\n- Security Clearance: User's level (Top Secret, Secret, Unclassified)\n- Security Classification: Data's level\n- Bell-LaPadula Model: \n  - No Read Up: Can't read higher classification\n  - No Write Down: Can't write to lower classification\n  - Prevents information leakage\n\nExample:\nUser with SECRET clearance:\n- Can read: UNCLASSIFIED, SECRET\n- Cannot read: TOP SECRET\n- Can write: SECRET, TOP SECRET\n- Cannot write: UNCLASSIFIED (prevents leaking)\n\nUsed in: Military, government systems\n\n3. ROLE-BASED ACCESS CONTROL (RBAC):\n\nPermissions assigned to roles, users assigned to roles\n\nAdvantages:\n- Easier management (modify role, not individual users)\n- Principle of least privilege\n- Separation of duties\n\nSQL Implementation:\n\nCREATE ROLE hr_manager;\nGRANT SELECT, UPDATE ON Employees TO hr_manager;\nGRANT SELECT ON Salaries TO hr_manager;\n\nCREATE ROLE accountant;\nGRANT SELECT ON Salaries TO accountant;\nGRANT INSERT, UPDATE ON Expenses TO accountant;\n\nGRANT hr_manager TO alice;\nGRANT accountant TO bob;\n\nRole Hierarchies:\nCREATE ROLE manager;\nGRANT hr_manager TO manager; -- Manager inherits HR permissions\n\nSQL INJECTION:\n\nAttack Mechanism:\nUser input interpreted as SQL code\n\nVulnerable Code (Python):\nusername = request.form['username']\npassword = request.form['password']\nquery = f\"SELECT * FROM Users WHERE username='{username}' AND password='{password}'\"\nexecute(query)\n\nAttack Input:\nusername: admin' --\npassword: anything\n\nResulting Query:\nSELECT * FROM Users WHERE username='admin' -- ' AND password='anything'\n(-- comments out rest, bypasses password check)\n\nWorse Attack (Union-based):\nusername: ' UNION SELECT * FROM CreditCards --\n\nResulting Query:\nSELECT * FROM Users WHERE username='' UNION SELECT * FROM CreditCards -- '\n(Returns credit card data!)\n\nPrevention:\n\n1. Parameterized Queries (Prepared Statements):\n\nSecure Code (Python):\nquery = \"SELECT * FROM Users WHERE username=? AND password=?\"\nexecute(query, (username, password))\n\nDatabase treats ? as data, not code\n\n2. Stored Procedures:\nCREATE PROCEDURE AuthenticateUser(@username NVARCHAR(50), @password NVARCHAR(50))\nAS\nBEGIN\n  SELECT * FROM Users WHERE username=@username AND password=@password;\nEND;\n\nCALL AuthenticateUser('admin', 'pass123');\n\n3. Input Validation:\n- Whitelist allowed characters\n- Reject suspicious input ('; --, UNION, etc.)\n- Escape special characters\n\n4. Least Privilege:\n- Application database account has minimal permissions\n- No DROP, CREATE permissions\n\n5. Web Application Firewall (WAF):\n- Detects and blocks SQL injection patterns\n\nENCRYPTION:\n\n1. Data at Rest:\n   - Encrypt database files\n   - Transparent Data Encryption (TDE)\n   - Column-level encryption for sensitive fields\n\n2. Data in Transit:\n   - SSL/TLS for connections\n   - HTTPS for web applications\n\n3. Key Management:\n   - Secure key storage (HSM, Key Vault)\n   - Key rotation policies\n\nAUDITING:\n\nTrack who accessed what, when\n\nAudit Log Contents:\n- User ID\n- Timestamp\n- Action (SELECT, INSERT, UPDATE, DELETE)\n- Table/row accessed\n- Success/failure\n- Source IP\n\nSQL Server Audit:\nCREATE SERVER AUDIT CompanyAudit\nTO FILE (FILEPATH = 'C:\\Audits\\')\nWITH (ON_FAILURE = CONTINUE);\n\nCREATE DATABASE AUDIT SPECIFICATION SalaryAudit\nFOR SERVER AUDIT CompanyAudit\nADD (SELECT, UPDATE ON Salaries BY public);\n\nBenefits:\n- Detect unauthorized access\n- Forensics after breach\n- Compliance (GDPR, HIPAA, SOX)",
          examples: "Example 1: SQL Injection Attack & Prevention\n\nVulnerable Login (PHP):\n$username = $_POST['username'];\n$password = $_POST['password'];\n$query = \"SELECT * FROM users WHERE username='$username' AND password='$password'\";\n$result = mysqli_query($conn, $query);\n\nAttack:\nUsername: admin' OR '1'='1\nPassword: anything\n\nQuery becomes:\nSELECT * FROM users WHERE username='admin' OR '1'='1' AND password='anything'\n(Always true, logs in as admin!)\n\nSecure Version (Prepared Statement):\n$stmt = $conn->prepare(\"SELECT * FROM users WHERE username=? AND password=?\");\n$stmt->bind_param(\"ss\", $username, $password);\n$stmt->execute();\n$result = $stmt->get_result();\n\nExample 2: RBAC Implementation\n\n-- Create roles\nCREATE ROLE sales_rep;\nCREATE ROLE sales_manager;\nCREATE ROLE admin;\n\n-- Grant permissions to roles\nGRANT SELECT ON Customers TO sales_rep;\nGRANT SELECT, INSERT ON Orders TO sales_rep;\n\nGRANT SELECT ON Customers TO sales_manager;\nGRANT SELECT, INSERT, UPDATE ON Orders TO sales_manager;\nGRANT SELECT ON SalesReport TO sales_manager;\n\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\n\n-- Assign users to roles\nGRANT sales_rep TO john, mary;\nGRANT sales_manager TO alice;\nGRANT admin TO bob;\n\n-- Role hierarchy (manager inherits rep permissions)\nGRANT sales_rep TO sales_manager;\n\nExample 3: Encryption\n\n-- Column-level encryption (SQL Server)\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'StrongPassword123!';\n\nCREATE CERTIFICATE SSNCert\nWITH SUBJECT = 'SSN Encryption Certificate';\n\nCREATE SYMMETRIC KEY SSNKey\nWITH ALGORITHM = AES_256\nENCRYPTION BY CERTIFICATE SSNCert;\n\n-- Encrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nINSERT INTO Employees (Name, SSN_Encrypted)\nVALUES ('John Doe', EncryptByKey(Key_GUID('SSNKey'), '123-45-6789'));\n\nCLOSE SYMMETRIC KEY SSNKey;\n\n-- Decrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nSELECT Name, CONVERT(VARCHAR, DecryptByKey(SSN_Encrypted)) AS SSN\nFROM Employees;\n\nCLOSE SYMMETRIC KEY SSNKey;\n\nExample 4: Auditing\n\n-- Enable auditing for sensitive table (PostgreSQL)\nCREATE TABLE audit_log (\n    audit_id SERIAL PRIMARY KEY,\n    table_name VARCHAR(50),\n    action VARCHAR(10),\n    user_name VARCHAR(50),\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    old_data JSON,\n    new_data JSON\n);\n\n-- Trigger to log changes\nCREATE OR REPLACE FUNCTION log_salary_changes()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'UPDATE' THEN\n        INSERT INTO audit_log (table_name, action, user_name, old_data, new_data)\n        VALUES ('Salaries', 'UPDATE', current_user, \n                row_to_json(OLD), row_to_json(NEW));\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER salary_audit\nAFTER UPDATE ON Salaries\nFOR EACH ROW EXECUTE FUNCTION log_salary_changes();",
          practical: "Best Practices:\n\n1. Defense in Depth:\n   - Multiple security layers\n   - Firewall + Authentication + Authorization + Encryption + Auditing\n   - If one fails, others still protect\n\n2. Principle of Least Privilege:\n   - Grant minimum necessary permissions\n   - Application account: Only needed tables\n   - Read-only users: No write permissions\n\n3. Input Validation:\n   - Validate all user input\n   - Use parameterized queries\n   - Whitelist, not blacklist\n\n4. Regular Security Audits:\n   - Review user permissions\n   - Check audit logs for anomalies\n   - Penetration testing\n\n5. Encryption:\n   - Encrypt sensitive data at rest\n   - Use SSL/TLS for connections\n   - Secure key management\n\n6. Patch Management:\n   - Keep database software updated\n   - Apply security patches promptly\n\n7. Monitoring & Alerting:\n   - Monitor failed login attempts\n   - Alert on unusual access patterns\n   - Real-time intrusion detection\n\nCompliance Standards:\n- GDPR: EU data protection\n- HIPAA: Healthcare data (US)\n- PCI DSS: Credit card data\n- SOX: Financial reporting\n\nReal-World Scenarios:\n- Healthcare: Encrypt patient records, audit all access\n- Banking: MFA, transaction auditing, encryption\n- E-commerce: PCI DSS compliance, secure payment data\n- Government: MAC for classified data",
          exam: "Important Questions:\n\n1. Explain the CIA triad in database security\n2. Compare DAC, MAC, and RBAC with examples\n3. What is SQL injection? How to prevent it?\n4. Show SQL injection attack and secure code\n5. Explain role-based access control with SQL examples\n6. What is the difference between authentication and authorization?\n7. Why use parameterized queries instead of string concatenation?\n8. Explain encryption at rest vs encryption in transit\n9. What is database auditing? Why is it important?\n10. Explain multi-factor authentication\n11. What is the principle of least privilege?\n12. How does GRANT and REVOKE work in SQL?",
          takeaways: "Key Principles:\n\n• Security requires multiple layers: authentication, authorization, encryption, auditing\n• SQL injection is preventable: use parameterized queries, never concatenate SQL\n• RBAC simplifies permission management: assign permissions to roles, not individuals\n• Principle of least privilege: grant minimum necessary permissions\n• Encryption protects data at rest and in transit\n• Auditing provides accountability and forensics\n• Defense in depth: multiple security layers provide redundancy\n• Security is ongoing: monitor, patch, audit regularly"
        }
      },
      {
        id: "advanced-topics",
        title: "Advanced Topics",
        subtopics: ["Object oriented and object relational databases", "Logical warehousing and data mining"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "Relational databases dominated for decades, but new challenges emerged: How to store complex objects? How to analyze petabytes of data? How to handle unstructured data? Advanced database technologies—object-oriented databases, data warehousing, NoSQL, and distributed systems—evolved to address these needs.",
          concept: "Advanced database topics extend beyond traditional relational databases to handle complex data types, massive-scale analytics, and modern application requirements.\n\nKey Motivations:\n\n1. Complex Data: Multimedia, CAD, scientific simulations\n2. Big Data: Petabytes of data, distributed processing\n3. Performance: Real-time analytics, low-latency access\n4. Scalability: Horizontal scaling, cloud-native\n5. Flexibility: Schema evolution, heterogeneous data",
          technicalDepth: "OBJECT-ORIENTED DATABASES (OODBMS):\n\nMotivation: Impedance mismatch between OOP languages and relational databases\n\nRelational Approach:\nclass Employee {\n    int id;\n    String name;\n    Address address; // Complex object\n}\n\n// Must decompose into multiple tables\nEmployees(id, name, address_id)\nAddresses(address_id, street, city, zip)\n\n// Retrieve requires joins, complex mapping\n\nOODBMS Approach:\nStore objects directly, no decomposition\nQuery using object-oriented syntax\n\nFeatures:\n\n1. Complex Objects:\n   - Nested objects\n   - Collections (lists, sets)\n   - References between objects\n\n2. Object Identity (OID):\n   - Each object has unique identifier\n   - Independent of attribute values\n   - Persists across sessions\n\n3. Encapsulation:\n   - Methods stored with data\n   - Behavior + data together\n\n4. Inheritance:\n   - Class hierarchies\n   - Subclasses inherit from superclasses\n\n5. Polymorphism:\n   - Method overriding\n   - Dynamic binding\n\nOODBMS Examples:\n- db4o (Java, .NET)\n- ObjectDB (Java)\n- Versant\n\nObject-Relational Databases (ORDBMS):\n\nHybrid: Relational model + OO features\n\nPostgreSQL Example:\n-- User-defined types\nCREATE TYPE Address AS (\n    street VARCHAR(100),\n    city VARCHAR(50),\n    zip VARCHAR(10)\n);\n\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    address Address  -- Complex type\n);\n\nINSERT INTO Employee VALUES (1, 'John', ROW('123 Main St', 'NYC', '10001'));\n\nSELECT name, (address).city FROM Employee;\n\nAdvantages:\n- Natural mapping from OOP\n- No joins for complex objects\n- Better performance for complex data\n\nDisadvantages:\n- Less mature than RDBMS\n- Smaller ecosystem\n- No standard query language (unlike SQL)\n\nDATA WAREHOUSING:\n\nDefinition: Subject-oriented, integrated, time-variant, non-volatile collection for decision support\n\nCharacteristics:\n\n1. Subject-Oriented: Organized by business area (Sales, Finance)\n2. Integrated: Data from multiple sources\n3. Time-Variant: Historical data, snapshots\n4. Non-Volatile: Read-only, no updates\n\nArchitecture:\n\nSource Systems (OLTP)\n    ↓\nETL (Extract, Transform, Load)\n    ↓\nData Warehouse\n    ↓\nData Marts (department-specific)\n    ↓\nOLAP / BI Tools\n\nOLTP vs OLAP:\n\nOLTP (Online Transaction Processing):\n- Current data\n- Detailed records\n- Read/Write\n- Normalized (3NF)\n- Fast transactions\n- Example: Bank account balance\n\nOLAP (Online Analytical Processing):\n- Historical data\n- Aggregated summaries\n- Read-mostly\n- Denormalized (star schema)\n- Complex queries\n- Example: Quarterly sales analysis\n\nDimensional Modeling:\n\nStar Schema:\n\n        Dimension: Time\n             |\n             |\nDimension -- Fact Table -- Dimension\n (Product)   (Sales)      (Customer)\n             |\n             |\n        Dimension: Store\n\nFact Table: Measures (sales amount, quantity)\nDimension Tables: Context (who, what, when, where)\n\nSnowflake Schema:\nNormalized dimensions (dimension tables have sub-dimensions)\n\nOLAP Operations:\n\n1. Roll-Up: Aggregate to higher level\n   Daily sales → Monthly sales\n\n2. Drill-Down: Detail to lower level\n   Yearly sales → Quarterly sales\n\n3. Slice: Fix one dimension\n   Sales in Q1 2024\n\n4. Dice: Fix multiple dimensions\n   Sales in Q1 2024, Region=West, Product=Laptop\n\n5. Pivot: Rotate view\n   Rows ↔ Columns\n\nDATA MINING:\n\nDefinition: Discovering patterns, correlations, trends in large datasets\n\nKnowledge Discovery Process (KDD):\n\n1. Data Cleaning: Remove noise, outliers\n2. Data Integration: Combine sources\n3. Data Selection: Choose relevant data\n4. Data Transformation: Normalize, aggregate\n5. Data Mining: Apply algorithms\n6. Pattern Evaluation: Identify interesting patterns\n7. Knowledge Presentation: Visualize, report\n\nData Mining Techniques:\n\n1. Association Rules:\n   Market Basket Analysis\n   {Bread, Butter} ⇒ {Milk} (support=30%, confidence=80%)\n   Support: % transactions containing itemset\n   Confidence: % transactions with antecedent that also have consequent\n\n2. Classification:\n   Assign items to predefined categories\n   Decision Trees, Neural Networks, SVM\n   Example: Spam vs Not Spam email\n\n3. Clustering:\n   Group similar items (no predefined categories)\n   K-Means, Hierarchical Clustering\n   Example: Customer segmentation\n\n4. Regression:\n   Predict continuous values\n   Linear Regression, Polynomial Regression\n   Example: Predict house price\n\n5. Anomaly Detection:\n   Identify outliers\n   Example: Fraud detection\n\n6. Sequential Patterns:\n   Find patterns over time\n   Example: Web clickstream analysis\n\nDISTRIBUTED DATABASES:\n\nData stored across multiple locations\n\nTypes:\n\n1. Homogeneous: Same DBMS at all sites\n2. Heterogeneous: Different DBMS at different sites\n\nFragmentation:\n\n1. Horizontal: Split rows\n   West_Customers (CA, WA, OR)\n   East_Customers (NY, MA, PA)\n\n2. Vertical: Split columns\n   Employee_Basic (id, name, dept)\n   Employee_Payroll (id, salary, bonus)\n\n3. Hybrid: Both\n\nReplication:\n- Full: Complete copy at each site\n- Partial: Selected fragments at sites\n\nAdvantages:\n- Improved performance (data locality)\n- Increased availability (redundancy)\n- Scalability\n\nChallenges:\n- Distributed query processing\n- Distributed transactions (2PC)\n- Consistency vs Availability (CAP theorem)\n\nNOSQL DATABASES:\n\nMotivation: Scalability, flexibility, performance for web-scale applications\n\nTypes:\n\n1. Key-Value: Redis, DynamoDB\n   Simple: key → value\n   Fast lookups\n   Use case: Session storage, caching\n\n2. Document: MongoDB, CouchDB\n   Store JSON/BSON documents\n   Schema-less\n   Use case: Content management, catalogs\n\n3. Column-Family: Cassandra, HBase\n   Wide columns, billions of columns\n   Distributed, high write throughput\n   Use case: Time-series, logs\n\n4. Graph: Neo4j, Amazon Neptune\n   Nodes and edges\n   Relationships are first-class\n   Use case: Social networks, recommendations\n\nCAP Theorem:\nCan have at most 2 of 3:\n- Consistency: All nodes see same data\n- Availability: Every request gets response\n- Partition Tolerance: Works despite network splits\n\nRelational: CA (sacrifice partition tolerance)\nNoSQL: Often AP or CP",
          examples: "Example 1: OODBMS (db4o - Java)\n\n// Define classes\nclass Employee {\n    String name;\n    Address address;\n    List<Project> projects;\n}\n\nclass Address {\n    String street;\n    String city;\n}\n\n// Store object directly\nEmployee emp = new Employee();\nemp.name = \"John\";\nemp.address = new Address(\"123 Main\", \"NYC\");\ndb.store(emp); // Stores entire object graph\n\n// Query\nList<Employee> result = db.query(new Predicate<Employee>() {\n    public boolean match(Employee emp) {\n        return emp.address.city.equals(\"NYC\");\n    }\n});\n\nExample 2: Data Warehouse Star Schema (SQL)\n\nCREATE TABLE FactSales (\n    sale_id INT PRIMARY KEY,\n    date_id INT,\n    product_id INT,\n    customer_id INT,\n    store_id INT,\n    quantity INT,\n    amount DECIMAL(10,2),\n    FOREIGN KEY (date_id) REFERENCES DimDate(date_id),\n    FOREIGN KEY (product_id) REFERENCES DimProduct(product_id),\n    FOREIGN KEY (customer_id) REFERENCES DimCustomer(customer_id),\n    FOREIGN KEY (store_id) REFERENCES DimStore(store_id)\n);\n\nCREATE TABLE DimDate (\n    date_id INT PRIMARY KEY,\n    date DATE,\n    day INT,\n    month INT,\n    quarter INT,\n    year INT\n);\n\nCREATE TABLE DimProduct (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    category VARCHAR(50),\n    brand VARCHAR(50)\n);\n\n-- Analytical Query: Quarterly sales by product category\nSELECT d.quarter, p.category, SUM(f.amount) AS total_sales\nFROM FactSales f\nJOIN DimDate d ON f.date_id = d.date_id\nJOIN DimProduct p ON f.product_id = p.product_id\nWHERE d.year = 2024\nGROUP BY d.quarter, p.category\nORDER BY d.quarter, total_sales DESC;\n\nExample 3: NoSQL (MongoDB)\n\n// Document database - store complex objects\ndb.employees.insertOne({\n    _id: 1,\n    name: \"John Doe\",\n    address: {\n        street: \"123 Main St\",\n        city: \"NYC\",\n        zip: \"10001\"\n    },\n    projects: [\n        { name: \"Project A\", role: \"Lead\" },\n        { name: \"Project B\", role: \"Developer\" }\n    ],\n    skills: [\"Java\", \"Python\", \"SQL\"]\n});\n\n// Query nested fields\ndb.employees.find({ \"address.city\": \"NYC\" });\n\n// Query arrays\ndb.employees.find({ skills: \"Python\" });\n\nExample 4: Data Mining - Association Rules (Python)\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport pandas as pd\n\n# Transaction data\ntransactions = [\n    ['Bread', 'Milk', 'Butter'],\n    ['Bread', 'Butter'],\n    ['Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk']\n]\n\n# Find frequent itemsets\nfrequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\nprint(rules[['antecedents', 'consequents', 'support', 'confidence']])\n\n# Output: {Bread, Butter} => {Milk} (confidence=0.8)",
          practical: "When to Use Each Technology:\n\n1. Relational Databases (PostgreSQL, MySQL):\n   - Structured data\n   - ACID transactions required\n   - Complex queries, joins\n   - Examples: Banking, e-commerce orders\n\n2. OODBMS / ORDBMS:\n   - Complex nested objects\n   - Multimedia, CAD, scientific data\n   - Tight OOP integration\n   - Examples: GIS, engineering simulations\n\n3. Data Warehouses:\n   - Historical analysis\n   - Business intelligence\n   - Reporting, dashboards\n   - Examples: Sales analytics, financial reporting\n\n4. NoSQL:\n   Document (MongoDB): Flexible schema, content management\n   Key-Value (Redis): Caching, session storage\n   Column-Family (Cassandra): Time-series, logs, high write throughput\n   Graph (Neo4j): Social networks, recommendations, fraud detection\n\n5. Distributed Databases:\n   - Global applications\n   - High availability requirements\n   - Scale beyond single server\n   - Examples: Multi-region applications\n\nModern Architecture Trends:\n- Polyglot Persistence: Use right database for each use case\n- Lambda Architecture: Batch + Stream processing\n- Data Lakes: Store raw data, process on-demand\n- Cloud Data Warehouses: Snowflake, BigQuery, Redshift",
          exam: "Key Questions:\n\n1. Compare relational databases with object-oriented databases\n2. What is impedance mismatch? How does OODBMS solve it?\n3. Explain star schema and snowflake schema\n4. What is the difference between OLTP and OLAP?\n5. Describe the ETL process in data warehousing\n6. Explain OLAP operations: roll-up, drill-down, slice, dice, pivot\n7. What is data mining? List and explain 3 techniques\n8. Explain association rule mining with example\n9. What is the CAP theorem? How does it relate to NoSQL?\n10. Compare document, key-value, column-family, and graph databases\n11. What is horizontal vs vertical fragmentation in distributed databases?\n12. When would you choose NoSQL over relational database?",
          takeaways: "Core Concepts:\n\n• OODBMS stores objects directly, eliminating impedance mismatch\n• Data warehouses support analytics with denormalized star/snowflake schemas\n• OLTP focuses on transactions, OLAP on analysis\n• Data mining discovers patterns: association rules, classification, clustering\n• NoSQL provides scalability and flexibility for specific use cases\n• CAP theorem: Can't have consistency, availability, and partition tolerance together\n• Distributed databases provide scalability through fragmentation and replication\n• Modern applications use polyglot persistence: right database for each use case"
        }
      }
    ]
  }
];
