export interface CLO {
  id: string;
  description: string;
}

export interface CO {
  id: string;
  description: string;
}

export interface Topic {
  id: string;
  title: string;
  subtopics?: string[];
  clos: string[];
  cos: string[];
  content: {
    introduction: string;
    concept: string;
    technicalDepth: string;
    examples: string;
    practical: string;
    exam: string;
    takeaways: string;
  };
}

export interface Unit {
  id: string;
  title: string;
  topics: Topic[];
}

export const clos: CLO[] = [
  { id: "CLO01", description: "Have a broad understanding of database concepts and database management system software" },
  { id: "CLO02", description: "Have a high-level understanding of major DBMS components and their function" },
  { id: "CLO03", description: "Be able to model an application's data requirements using conceptual modeling tools like ER diagrams and design database schemas based on the conceptual model." },
  { id: "CLO04", description: "Be able to write SQL commands to create tables and indexes, insert/update/delete data, and query data in a relational DBMS." },
  { id: "CLO05", description: "Be able to program a data-intensive application using DBMS APIs." },
];

export const cos: CO[] = [
  { id: "CO01", description: "Identify the basic concepts and various data model used in database design ER modelling concepts and architecture use and design queries using SQL" },
  { id: "CO02", description: "Apply relational database theory and be able to describe relational algebra expression, tuple and domain relation expression from queries." },
  { id: "CO03", description: "Recognize and identify the use of normalization and functional dependency. indexing and hashing technique used in database design." },
  { id: "CO04", description: "Recognize/ Identify the purpose of query processing and optimization and also demonstrate the basic of query evaluation." },
  { id: "CO05", description: "Apply and relate the concept of transaction, concurrency control and recovery in database." },
  { id: "CO06", description: "Understanding of recovery system and be familiar with introduction to web database, distribute databases, data warehousing and mining." },
];

export const units: Unit[] = [
  {
    id: "unit-1",
    title: "UNIT I: Introduction & Architecture",
    topics: [
      {
        id: "intro-to-db",
        title: "Introduction to Database",
        subtopics: ["Hierarchical, Network and Relational database"],
        clos: ["CLO01"],
        cos: ["CO01"],
        content: {
          introduction: "Database systems form the backbone of modern information management, handling everything from banking transactions to social media interactions. A Database Management System (DBMS) is a collection of interrelated data and a set of programs to access that data, providing a convenient and efficient way to store and retrieve database information. The primary goal is to manage data in a way that minimizes redundancy and ensures data integrity while allowing multiple users concurrent access.",
          concept: "A database is a collection of related data organized to serve multiple applications efficiently. Data represents recordable facts with implicit meaning, such as names, addresses, and balances. A DBMS serves as an interface between the database and end users or application programs.\n\nKey Characteristics of Database Approach:\n\n1. Self-describing nature: Database contains not only data but also metadata (data about data) in the system catalog\n\n2. Insulation between programs and data: Program-data independence allows changing data structure without modifying programs\n\n3. Support for multiple views: Different users can have different perspectives of the same data\n\n4. Sharing of data and multiuser transaction processing: Multiple users can access database simultaneously while maintaining consistency\n\nAdvantages of Using DBMS:\nâ€¢ Controlling redundancy: Eliminates duplicate data storage\nâ€¢ Restricting unauthorized access: Security and authorization mechanisms\nâ€¢ Providing persistent storage: Data outlives programs that create it\nâ€¢ Providing backup and recovery: Protection against hardware/software failures\nâ€¢ Enforcing integrity constraints: Rules to maintain data accuracy\nâ€¢ Permitting inference and actions using rules: Database triggers and stored procedures",
          technicalDepth: "DATABASE MODELS EVOLUTION:\n\n1. HIERARCHICAL MODEL (1960s-1970s):\nStructure: Tree-like hierarchy with parent-child relationships\nCharacteristics:\n  â€¢ One-to-many (1:N) relationships only\n  â€¢ Single root for each tree\n  â€¢ Each child has exactly one parent\n  â€¢ Navigation through explicit paths\n\nExample: IMS (Information Management System) by IBM\nLimitations:\n  â€¢ Difficulty handling many-to-many relationships\n  â€¢ Data redundancy when representing complex relationships\n  â€¢ Limited query flexibility\n  â€¢ Reorganization is difficult\n\n2. NETWORK MODEL (1970s):\nStructure: Graph-based with records (nodes) and sets (edges)\nCharacteristics:\n  â€¢ Many-to-many (M:N) relationships supported\n  â€¢ Records connected through pointers\n  â€¢ Set construct for 1:N relationships\n  â€¢ CODASYL (Conference on Data Systems Languages) standard\n\nExample: IDMS (Integrated Database Management System)\nAdvantages over Hierarchical:\n  â€¢ More flexible relationship representation\n  â€¢ Better for complex queries\nLimitations:\n  â€¢ Complex navigation logic\n  â€¢ Difficult to modify structure\n  â€¢ Application programs tightly coupled with database structure\n\n3. RELATIONAL MODEL (1970-present):\nFoundation: E.F. Codd's relational model (1970)\nStructure: Data organized in tables (relations)\n\nCore Concepts:\n  â€¢ Relation: Table with rows (tuples) and columns (attributes)\n  â€¢ Domain: Set of allowed values for an attribute\n  â€¢ Primary Key: Unique identifier for tuples\n  â€¢ Foreign Key: Reference to primary key in another relation\n\nCharacteristics:\n  â€¢ Simple tabular structure\n  â€¢ Data independence (logical and physical)\n  â€¢ Declarative query language (SQL)\n  â€¢ Mathematical foundation (relational algebra/calculus)\n  â€¢ ACID properties (Atomicity, Consistency, Isolation, Durability)\n\nAdvantages:\n  â€¢ Structural independence: Changes in structure don't affect applications\n  â€¢ Ad hoc query capability: Flexible data retrieval\n  â€¢ Data integrity through constraints\n  â€¢ Normalization to reduce redundancy\n\nExamples: MySQL, PostgreSQL, Oracle, SQL Server, DB2\n\nCOMPARATIVE ANALYSIS:\n\nFeature          | Hierarchical | Network  | Relational\n-----------------|--------------|----------|-------------\nStructure        | Tree         | Graph    | Tables\nRelationships    | 1:N          | M:N      | Any\nNavigation       | Procedural   | Procedural| Declarative\nFlexibility      | Low          | Medium   | High\nQuery Language   | Proprietary  | Proprietary| SQL (Standard)\nData Independence| Low          | Low      | High\nPerformance      | Fast         | Fast     | Moderate\n\nWhy Relational Model Dominates:\n1. Simplicity: Easy to understand and use\n2. Flexibility: Ad hoc queries without predefined access paths\n3. Data Independence: Separation of logical and physical views\n4. Standard Language: SQL is universally adopted\n5. Mathematical Foundation: Formal query optimization\n6. ACID Guarantees: Reliable transaction processing",
          examples: "EXAMPLE 1: HIERARCHICAL DATABASE\n\nUniversity Structure (Tree):\n                University\n                    |\n        -------------------------\n        |                       |\n    Department              Department\n   (Computer Science)        (Mathematics)\n        |                       |\n    ----------              ----------\n    |        |              |        |\n  Faculty  Students      Faculty  Students\n\nLimitation: A student double-majoring requires duplication or complex workarounds.\n\nEXAMPLE 2: NETWORK DATABASE\n\nStudent-Course Registration (Graph):\nStudents <-------- Enrollment --------> Courses\n   |                    |                   |\n   |              (Properties: Grade,       |\n   |               Semester)                |\n   |                                        |\nDepartments                          Instructors\n\nNetwork allows M:N relationships directly but requires complex pointer navigation.\n\nEXAMPLE 3: RELATIONAL DATABASE\n\nUniversity Schema (Tables):\n\nSTUDENTS Table:\nStudentID | Name        | Major    | Email\n----------|-------------|----------|------------------\n101       | John Doe    | CS       | john@uni.edu\n102       | Jane Smith  | Math     | jane@uni.edu\n103       | Bob Johnson | CS       | bob@uni.edu\n\nCOURSES Table:\nCourseID | CourseName           | Credits | Department\n---------|---------------------|---------|------------\nCS101    | Intro to Programming | 3       | CS\nCS202    | Database Systems     | 4       | CS\nMATH101  | Calculus I          | 4       | Math\n\nENROLLMENT Table (Junction):\nStudentID | CourseID | Semester | Grade\n----------|----------|----------|------\n101       | CS101    | Fall2024 | A\n101       | CS202    | Fall2024 | B+\n102       | MATH101  | Fall2024 | A\n103       | CS101    | Fall2024 | B\n\nSQL Query Examples:\n\n-- Find all students enrolled in Database Systems\nSELECT S.Name, S.Email\nFROM STUDENTS S\nJOIN ENROLLMENT E ON S.StudentID = E.StudentID\nJOIN COURSES C ON E.CourseID = C.CourseID\nWHERE C.CourseName = 'Database Systems';\n\n-- Count enrollments per course\nSELECT C.CourseName, COUNT(E.StudentID) as Enrollment_Count\nFROM COURSES C\nLEFT JOIN ENROLLMENT E ON C.CourseID = E.CourseID\nGROUP BY C.CourseName;\n\nREAL-WORLD APPLICATIONS:\n\n1. Banking Systems: Account management, transactions, customer data\n   â€¢ Relational model ensures ACID properties for financial transactions\n   â€¢ Complex queries for fraud detection and reporting\n\n2. Airline Reservation Systems: Flight schedules, bookings, passenger info\n   â€¢ High concurrency handling thousands of simultaneous bookings\n   â€¢ Complex relationships between flights, passengers, seats\n\n3. E-Commerce Platforms: Products, orders, customers, inventory\n   â€¢ Flexible schema for diverse product catalogs\n   â€¢ Transaction support for payment processing\n\n4. Healthcare Systems: Patient records, appointments, prescriptions\n   â€¢ Data integrity critical for patient safety\n   â€¢ HIPAA compliance through access control\n\nðŸ“š STUDY MATERIALS:\nâ€¢ Unit 1 Notes: <a href=\"/study-materials/Unit 1.pdf\" target=\"_blank\" class=\"text-blue-600 hover:underline\">Download PDF</a>\nâ€¢ Textbook Reference: Silberschatz Chapter 1-2 <a href=\"/study-materials/Database System Concepts - Silberschatz-Database System Concepts 6th ed.pdf\" target=\"_blank\" class=\"text-blue-600 hover:underline\">View Textbook</a>",
          practical: "PRACTICAL USAGE AND IMPLEMENTATION:\n\n1. Choosing the Right Model:\n   â€¢ Use Relational for: General-purpose applications, ACID requirements, complex queries\n   â€¢ Consider NoSQL (modern evolution) for: Massive scale, flexible schema, specific access patterns\n\n2. Database Design Process:\n   Step 1: Requirements Analysis - Understand what data to store and how it will be used\n   Step 2: Conceptual Design - Create ER diagram showing entities and relationships\n   Step 3: Logical Design - Convert ER to relational schema with normalization\n   Step 4: Physical Design - Optimize with indexes, partitioning, clustering\n\n3. Real Implementation Example (MySQL):\n\n-- Create database\nCREATE DATABASE UniversityDB;\nUSE UniversityDB;\n\n-- Create tables with constraints\nCREATE TABLE Students (\n    StudentID INT PRIMARY KEY AUTO_INCREMENT,\n    Name VARCHAR(100) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    Major VARCHAR(50),\n    EnrollmentDate DATE DEFAULT CURRENT_DATE\n);\n\nCREATE TABLE Courses (\n    CourseID VARCHAR(10) PRIMARY KEY,\n    CourseName VARCHAR(100) NOT NULL,\n    Credits INT CHECK (Credits > 0),\n    Department VARCHAR(50)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID VARCHAR(10),\n    Semester VARCHAR(20),\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID, Semester),\n    FOREIGN KEY (StudentID) REFERENCES Students(StudentID)\n        ON DELETE CASCADE,\n    FOREIGN KEY (CourseID) REFERENCES Courses(CourseID)\n        ON UPDATE CASCADE\n);\n\n4. Industry Best Practices:\n   â€¢ Always use foreign keys to maintain referential integrity\n   â€¢ Create indexes on frequently queried columns\n   â€¢ Regular backups and disaster recovery plans\n   â€¢ Monitor performance and optimize queries\n   â€¢ Implement proper access control and authentication\n\n5. Career Relevance:\n   â€¢ Database Administrator (DBA): Manages database systems, ensures performance and security\n   â€¢ Data Engineer: Designs and builds data pipelines\n   â€¢ Backend Developer: Implements database interactions in applications\n   â€¢ Data Analyst: Writes complex SQL queries for business insights",
          exam: "IMPORTANT EXAM QUESTIONS:\n\n1. Define Database and DBMS. What are the main characteristics of the database approach?\n   Answer: Cover self-describing nature, program-data independence, multiple views, sharing\n\n2. List and explain the advantages of using a DBMS over file systems.\n   Key points: Redundancy control, restricted access, persistent storage, backup/recovery, integrity constraints\n\n3. Compare and contrast Hierarchical, Network, and Relational database models.\n   Create table showing: Structure, relationships supported, advantages, disadvantages, examples\n\n4. Why did the Relational Model become dominant over Hierarchical and Network models?\n   Focus on: Simplicity, data independence, SQL standardization, flexibility\n\n5. Explain the concept of data independence with examples.\n   Distinguish between logical and physical data independence\n\n6. What are the disadvantages of file-based systems that led to database systems?\n   Points: Data redundancy, inconsistency, difficulty in access, integrity problems, atomicity issues, security problems\n\n7. Describe the structure and limitations of the Hierarchical database model.\n   Include: Tree structure, 1:N relationships, navigation, IMS example\n\n8. Draw a simple example showing how M:N relationships are handled differently in Hierarchical, Network, and Relational models.\n\n9. What is a view in a database? Why are multiple views important?\n   Explain: Security, customization, simplicity for different user groups\n\n10. Explain the term 'self-describing' nature of a database system.\n    Focus on: System catalog, metadata, data dictionary\n\nQUICK REVISION POINTS:\nâ€¢ Database = Collection of related data\nâ€¢ DBMS = Software to manage database\nâ€¢ Hierarchical = Tree (1:N only)\nâ€¢ Network = Graph (M:N supported)\nâ€¢ Relational = Tables (Most flexible)\nâ€¢ SQL = Standard query language for relational databases\nâ€¢ Data Independence = Change structure without affecting programs",
          takeaways: "KEY TAKEAWAYS:\n\nâœ“ A database is a collection of related data; DBMS is the software that manages it\n\nâœ“ Database approach provides: data independence, controlled redundancy, concurrent access, backup/recovery, security\n\nâœ“ Three major historical models: Hierarchical (tree), Network (graph), Relational (tables)\n\nâœ“ Hierarchical model: Simple but limited to 1:N relationships, used in IBM's IMS\n\nâœ“ Network model: More flexible with M:N relationships but complex navigation\n\nâœ“ Relational model: Dominant today due to simplicity, SQL standardization, and data independence\n\nâœ“ Relational databases organize data in tables with rows (tuples) and columns (attributes)\n\nâœ“ SQL provides declarative querying - specify WHAT you want, not HOW to get it\n\nâœ“ Modern applications almost exclusively use relational or NoSQL (evolution of database concepts)\n\nâœ“ Understanding database evolution helps appreciate why certain design decisions were made\n\nâœ“ Database systems are critical infrastructure for: banking, healthcare, e-commerce, government, education\n\nâœ“ Career opportunities: DBA, Data Engineer, Backend Developer, Data Analyst all require database knowledge\n\nREMEMBER: The shift from Hierarchical â†’ Network â†’ Relational represents increasing flexibility and decreasing complexity for users, while the system handles complexity internally."
        }
      },
      {
        id: "db-architecture",
        title: "Database System Architecture",
        subtopics: ["Data abstraction", "Data independence", "DDL", "DML", "DCL"],
        clos: ["CLO02"],
        cos: ["CO01"],
        content: {
          introduction: "Database architecture defines how a database is structured, stored, managed, and accessed. The architecture provides different views to different users and handles the complexity of physical storage while presenting a simple logical view. Understanding this layered architecture is fundamental to designing scalable, maintainable database systems that can evolve with changing requirements without disrupting applications.",
          concept: "A database system is divided into modules that deal with different responsibilities. The overall system structure consists of various components that interact to provide the database services.\n\nTHREE-SCHEMA ARCHITECTURE (ANSI-SPARC):\n\nThe three-schema architecture separates the user applications from the physical database through three levels of abstraction:\n\n1. EXTERNAL LEVEL (View Level):\n   â€¢ Highest level of abstraction\n   â€¢ Describes WHAT data is seen by individual users\n   â€¢ Multiple external schemas (views) exist\n   â€¢ Each view describes the portion of database relevant to a particular user group\n   â€¢ Hides irrelevant details and restricts access\n   â€¢ Different users can have completely different views\n\nExample: HR Manager sees employee salaries, Regular employee doesn't\n\n2. CONCEPTUAL LEVEL (Logical Level):\n   â€¢ Community view of the database\n   â€¢ Describes WHAT data is stored and relationships among data\n   â€¢ Single conceptual schema for entire database\n   â€¢ Hides details of physical storage\n   â€¢ Focus on entities, data types, relationships, constraints\n   â€¢ Used by Database Administrators (DBAs)\n\nExample: Complete schema with all tables, relationships, constraints\n\n3. INTERNAL LEVEL (Physical Level):\n   â€¢ Lowest level of abstraction\n   â€¢ Describes HOW data is physically stored\n   â€¢ Deals with: Storage structures, indexes, access paths, file organization\n   â€¢ Concerned with: Storage space allocation, data compression, encryption\n   â€¢ Used by system programmers and DBAs\n\nExample: B-tree indexes, heap files, hash buckets, block sizes\n\nMAPPINGS BETWEEN LEVELS:\nâ€¢ External/Conceptual Mapping: Transforms external view requests to conceptual schema\nâ€¢ Conceptual/Internal Mapping: Maps conceptual schema to physical storage\n\nDATA INDEPENDENCE:\n\nThe capacity to change schema at one level without affecting schema at the next higher level.\n\n1. LOGICAL DATA INDEPENDENCE:\n   â€¢ Ability to change conceptual schema without changing external schemas or application programs\n   â€¢ Changes might include: Adding/removing attributes, creating/removing tables, changing constraints\n   â€¢ External/Conceptual mapping handles the conversion\n   â€¢ More difficult to achieve than physical independence\n\nExample: Adding a new table or column doesn't affect existing views\n\n2. PHYSICAL DATA INDEPENDENCE:\n   â€¢ Ability to change internal schema without changing conceptual schema\n   â€¢ Changes might include: Using different file organization, creating/dropping indexes, changing storage devices, modifying data compression\n   â€¢ Conceptual/Internal mapping handles the conversion\n   â€¢ Easier to achieve than logical independence\n\nExample: Creating an index on a column doesn't affect queries\n\nBENEFITS OF DATA INDEPENDENCE:\nâ€¢ Application programs don't need to be rewritten when database structure changes\nâ€¢ Physical storage can be optimized without affecting applications\nâ€¢ Different users can view data differently\nâ€¢ Security: Users only see authorized data\nâ€¢ Maintenance and evolution of database is easier",
          technicalDepth: "DATABASE LANGUAGES:\n\nDatabase systems provide specialized languages for different operations:\n\n1. DATA DEFINITION LANGUAGE (DDL):\n\nUsed to define and modify database structure (schema).\n\nKey DDL Commands:\nâ€¢ CREATE: Define new database objects (tables, indexes, views)\nâ€¢ ALTER: Modify existing database objects\nâ€¢ DROP: Delete database objects\nâ€¢ TRUNCATE: Remove all records from a table\nâ€¢ RENAME: Rename database objects\n\nDDL Compiler:\nâ€¢ Processes DDL statements\nâ€¢ Generates metadata stored in data dictionary (system catalog)\nâ€¢ Data dictionary contains: Table definitions, attribute definitions, constraints, indexes, storage information\n\nDDL Features:\nâ€¢ Specify storage structure and access methods\nâ€¢ Define integrity constraints (primary keys, foreign keys, check constraints)\nâ€¢ Define security and authorization information\nâ€¢ Specify physical storage parameters\n\n2. DATA MANIPULATION LANGUAGE (DML):\n\nUsed to access and manipulate data in the database.\n\nTypes of DML:\n\nA. PROCEDURAL DML:\n   â€¢ User specifies WHAT data is needed AND HOW to get it\n   â€¢ Requires step-by-step navigation\n   â€¢ Examples: Hierarchical and Network model languages\n\nB. NON-PROCEDURAL (DECLARATIVE) DML:\n   â€¢ User specifies WHAT data is needed without specifying HOW\n   â€¢ System determines efficient way to access data\n   â€¢ Easier to learn and use\n   â€¢ Example: SQL (Structured Query Language)\n\nKey DML Commands (SQL):\nâ€¢ SELECT: Retrieve data from database\nâ€¢ INSERT: Add new records\nâ€¢ UPDATE: Modify existing records\nâ€¢ DELETE: Remove records\n\nDML Processing:\nâ€¢ DML Compiler: Translates DML statements\nâ€¢ Query Optimizer: Determines efficient execution strategy\nâ€¢ Query Evaluation Engine: Executes the optimized query plan\n\n3. DATA CONTROL LANGUAGE (DCL):\n\nUsed to control access to data in the database.\n\nKey DCL Commands:\nâ€¢ GRANT: Give user access privileges\nâ€¢ REVOKE: Remove user access privileges\n\nDCL Controls:\nâ€¢ User authentication\nâ€¢ Authorization levels\nâ€¢ Role-based access control\nâ€¢ Object-level permissions\n\n4. TRANSACTION CONTROL LANGUAGE (TCL):\n\nManages transactions in the database.\n\nKey TCL Commands:\nâ€¢ COMMIT: Save transaction changes permanently\nâ€¢ ROLLBACK: Undo transaction changes\nâ€¢ SAVEPOINT: Create points within transaction to rollback to\n\nDATABASE SYSTEM COMPONENTS:\n\n1. STORAGE MANAGER:\n   â€¢ Interface between low-level data and application programs\n   â€¢ Translates DML statements to low-level file-system commands\n   â€¢ Responsible for: Storing, retrieving, updating data\n   \n   Components:\n   â€¢ Authorization and Integrity Manager\n   â€¢ Transaction Manager\n   â€¢ File Manager\n   â€¢ Buffer Manager\n\n2. QUERY PROCESSOR:\n   â€¢ Simplifies and facilitates data access\n   \n   Components:\n   â€¢ DDL Interpreter: Interprets DDL statements\n   â€¢ DML Compiler: Translates DML to query evaluation plan\n   â€¢ Query Optimizer: Chooses lowest-cost evaluation plan\n   â€¢ Query Evaluation Engine: Executes query plan\n\n3. DATABASE USERS:\n\n   A. Naive Users: Invoke application programs (ATM users, airline booking)\n   B. Application Programmers: Write application programs using DML\n   C. Sophisticated Users: Use query tools, don't write programs (analysts)\n   D. Database Administrator (DBA): Central control over system\n\n4. DBA RESPONSIBILITIES:\n   â€¢ Schema definition\n   â€¢ Storage structure and access method definition\n   â€¢ Schema and physical organization modification\n   â€¢ Granting authorization for data access\n   â€¢ Routine maintenance: Backups, performance monitoring, security\n\nCLIENT-SERVER ARCHITECTURE:\n\nTwo-Tier Architecture:\nâ€¢ Client: Application program, user interface\nâ€¢ Server: Database system (query processing, transaction management)\nâ€¢ Communication via database protocols (ODBC, JDBC)\n\nThree-Tier Architecture:\nâ€¢ Client: Presentation layer (browser, mobile app)\nâ€¢ Application Server: Business logic layer (web server, API)\nâ€¢ Database Server: Data layer\nâ€¢ Benefits: Scalability, maintainability, security\n\nModern web applications typically use three-tier architecture.",
          examples: "EXAMPLE 1: DDL STATEMENTS (Creating Database Schema)\n\n-- Create a database\nCREATE DATABASE UniversityDB;\nUSE UniversityDB;\n\n-- Create Department table\nCREATE TABLE Department (\n    DeptID INT PRIMARY KEY AUTO_INCREMENT,\n    DeptName VARCHAR(100) NOT NULL UNIQUE,\n    Building VARCHAR(50),\n    Budget DECIMAL(12,2) CHECK (Budget > 0)\n);\n\n-- Create Student table with constraints\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    FirstName VARCHAR(50) NOT NULL,\n    LastName VARCHAR(50) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    DeptID INT,\n    EnrollmentDate DATE DEFAULT CURRENT_DATE,\n    GPA DECIMAL(3,2) CHECK (GPA >= 0.0 AND GPA <= 4.0),\n    FOREIGN KEY (DeptID) REFERENCES Department(DeptID)\n        ON DELETE SET NULL\n        ON UPDATE CASCADE\n);\n\n-- Create index for faster queries\nCREATE INDEX idx_student_lastname ON Student(LastName);\n\n-- Alter table to add new column\nALTER TABLE Student\nADD COLUMN PhoneNumber VARCHAR(15);\n\n-- Modify column definition\nALTER TABLE Student\nMODIFY COLUMN Email VARCHAR(150);\n\n-- Drop index\nDROP INDEX idx_student_lastname ON Student;\n\n-- Rename table\nRENAME TABLE Student TO Students;\n\nEXAMPLE 2: DML STATEMENTS (Data Manipulation)\n\n-- INSERT: Adding new records\nINSERT INTO Department (DeptName, Building, Budget)\nVALUES ('Computer Science', 'Tech Building', 500000.00);\n\nINSERT INTO Department VALUES \n    (NULL, 'Mathematics', 'Science Hall', 300000.00),\n    (NULL, 'Physics', 'Science Hall', 450000.00);\n\nINSERT INTO Students (StudentID, FirstName, LastName, Email, DeptID, GPA)\nVALUES (101, 'John', 'Doe', 'john.doe@uni.edu', 1, 3.75);\n\n-- SELECT: Retrieving data\n-- Simple query\nSELECT * FROM Students;\n\n-- With WHERE clause\nSELECT FirstName, LastName, GPA\nFROM Students\nWHERE GPA > 3.5;\n\n-- With JOIN\nSELECT S.FirstName, S.LastName, D.DeptName\nFROM Students S\nJOIN Department D ON S.DeptID = D.DeptID;\n\n-- With aggregation\nSELECT D.DeptName, COUNT(S.StudentID) as StudentCount, AVG(S.GPA) as AvgGPA\nFROM Department D\nLEFT JOIN Students S ON D.DeptID = S.DeptID\nGROUP BY D.DeptName\nHAVING COUNT(S.StudentID) > 0;\n\n-- UPDATE: Modifying existing records\nUPDATE Students\nSET GPA = 3.85\nWHERE StudentID = 101;\n\n-- Update with calculation\nUPDATE Department\nSET Budget = Budget * 1.10\nWHERE DeptName = 'Computer Science';\n\n-- DELETE: Removing records\nDELETE FROM Students\nWHERE EnrollmentDate < '2020-01-01';\n\n-- Delete all records (but keep table structure)\nDELETE FROM Students;\n-- Or use TRUNCATE (faster)\nTRUNCATE TABLE Students;\n\nEXAMPLE 3: DCL STATEMENTS (Access Control)\n\n-- Create users\nCREATE USER 'student_user'@'localhost' IDENTIFIED BY 'password123';\nCREATE USER 'faculty_user'@'localhost' IDENTIFIED BY 'securepass456';\n\n-- GRANT privileges\n-- Give SELECT access to student\nGRANT SELECT ON UniversityDB.Students TO 'student_user'@'localhost';\n\n-- Give multiple privileges to faculty\nGRANT SELECT, INSERT, UPDATE ON UniversityDB.* TO 'faculty_user'@'localhost';\n\n-- Grant with ability to grant to others\nGRANT ALL PRIVILEGES ON UniversityDB.* TO 'admin_user'@'localhost' \nWITH GRANT OPTION;\n\n-- Create role and assign privileges\nCREATE ROLE 'data_analyst';\nGRANT SELECT ON UniversityDB.* TO 'data_analyst';\nGRANT 'data_analyst' TO 'analyst_user'@'localhost';\n\n-- REVOKE privileges\nREVOKE INSERT, UPDATE ON UniversityDB.Students FROM 'faculty_user'@'localhost';\n\n-- Revoke all privileges\nREVOKE ALL PRIVILEGES ON UniversityDB.* FROM 'student_user'@'localhost';\n\nEXAMPLE 4: TCL STATEMENTS (Transaction Control)\n\n-- Bank transfer example demonstrating transaction\nSTART TRANSACTION;\n\n-- Deduct from account A\nUPDATE Accounts\nSET Balance = Balance - 500\nWHERE AccountID = 'A123';\n\n-- Add to account B\nUPDATE Accounts\nSET Balance = Balance + 500\nWHERE AccountID = 'B456';\n\n-- Check if any account went negative\nIF (SELECT MIN(Balance) FROM Accounts WHERE AccountID IN ('A123', 'B456')) < 0 THEN\n    ROLLBACK;  -- Undo both updates\n    SELECT 'Transaction failed: Insufficient funds' AS Message;\nELSE\n    COMMIT;    -- Make changes permanent\n    SELECT 'Transaction successful' AS Message;\nEND IF;\n\n-- Using SAVEPOINT\nSTART TRANSACTION;\n\nINSERT INTO Students VALUES (201, 'Alice', 'Smith', 'alice@uni.edu', 1, 3.5);\nSAVEPOINT sp1;\n\nINSERT INTO Students VALUES (202, 'Bob', 'Johnson', 'bob@uni.edu', 2, 3.2);\nSAVEPOINT sp2;\n\nINSERT INTO Students VALUES (203, 'Charlie', 'Brown', 'charlie@uni.edu', 1, 3.9);\n\n-- Error in last insert, rollback to sp2\nROLLBACK TO SAVEPOINT sp2;\n\n-- Commit the first two inserts\nCOMMIT;\n\nEXAMPLE 5: DATA INDEPENDENCE IN ACTION\n\nSCENARIO: Physical Data Independence\n\n-- Original: No index, slow query\nSELECT * FROM Students WHERE LastName = 'Smith';\n-- Query scans entire table (slow for large tables)\n\n-- DBA creates index (internal level change)\nCREATE INDEX idx_lastname ON Students(LastName);\n\n-- Same query, now fast (uses index)\nSELECT * FROM Students WHERE LastName = 'Smith';\n-- Application code unchanged, but performance improved!\n\nSCENARIO: Logical Data Independence\n\n-- Original view used by application\nCREATE VIEW StudentInfo AS\nSELECT StudentID, FirstName, LastName, Email\nFROM Students;\n\n-- DBA adds new column to base table (conceptual level change)\nALTER TABLE Students ADD COLUMN MiddleName VARCHAR(50);\n\n-- View still works, application unaffected\nSELECT * FROM StudentInfo;\n-- Returns same columns as before\n\nðŸ“š STUDY MATERIALS:\nâ€¢ Unit 1 Notes: <a href=\"/study-materials/Unit 1.pdf\" target=\"_blank\" class=\"text-blue-600 hover:underline\">Download PDF</a>\nâ€¢ Textbook Reference: Silberschatz Chapter 1-2 <a href=\"/study-materials/Database System Concepts - Silberschatz-Database System Concepts 6th ed.pdf\" target=\"_blank\" class=\"text-blue-600 hover:underline\">View Textbook</a>",
          practical: "REAL-WORLD APPLICATIONS AND BEST PRACTICES:\n\n1. THREE-TIER WEB APPLICATION EXAMPLE:\n\nPresentation Tier (Frontend):\n  â€¢ React/Angular/Vue application\n  â€¢ Displays data to user\n  â€¢ Sends API requests\n\nApplication Tier (Backend):\n  â€¢ Node.js/Python/Java server\n  â€¢ Business logic\n  â€¢ Validates data\n  â€¢ Manages database connections\n  â€¢ Uses connection pooling for efficiency\n\nDatabase Tier:\n  â€¢ MySQL/PostgreSQL/Oracle\n  â€¢ Stores persistent data\n  â€¢ Enforces constraints\n  â€¢ Handles transactions\n\nExample Code (Node.js + MySQL):\n\nconst mysql = require('mysql2');\n\n// Create connection pool (reuses connections)\nconst pool = mysql.createPool({\n  host: 'localhost',\n  user: 'app_user',\n  password: 'secure_password',\n  database: 'UniversityDB',\n  waitForConnections: true,\n  connectionLimit: 10\n});\n\n// API endpoint using DML\napp.get('/api/students', async (req, res) => {\n  const [rows] = await pool.promise().query(\n    'SELECT StudentID, FirstName, LastName, GPA FROM Students WHERE GPA > ?',\n    [3.0]\n  );\n  res.json(rows);\n});\n\napp.post('/api/students', async (req, res) => {\n  const { firstName, lastName, email, deptId } = req.body;\n  const [result] = await pool.promise().query(\n    'INSERT INTO Students (FirstName, LastName, Email, DeptID) VALUES (?, ?, ?, ?)',\n    [firstName, lastName, email, deptId]\n  );\n  res.json({ studentId: result.insertId, message: 'Student created' });\n});\n\n2. IMPLEMENTING DATA INDEPENDENCE:\n\nPhysical Independence Example:\n-- Application uses view, not table directly\nCREATE VIEW ActiveStudents AS\nSELECT StudentID, FirstName, LastName, Email, GPA\nFROM Students\nWHERE Status = 'Active';\n\n-- DBA can change physical storage without affecting app\nCREATE INDEX idx_status ON Students(Status); -- Add index\nALTER TABLE Students ENGINE=InnoDB; -- Change storage engine\nPARTITION TABLE Students BY RANGE(EnrollmentYear); -- Partition table\n\n-- Application code unchanged:\nSELECT * FROM ActiveStudents; -- Still works!\n\n3. DBA DAILY TASKS:\n\nMorning:\n  â€¢ Check database server status\n  â€¢ Review overnight backup logs\n  â€¢ Monitor disk space usage\n  â€¢ Check for slow queries\n\nDuring Day:\n  â€¢ Create/modify schemas as requested\n  â€¢ Grant/revoke user privileges\n  â€¢ Investigate performance issues\n  â€¢ Optimize slow queries with indexes\n\nEnd of Day:\n  â€¢ Schedule backups\n  â€¢ Review security logs\n  â€¢ Plan maintenance windows\n\n4. SECURITY BEST PRACTICES:\n\nPrinciple of Least Privilege:\n-- Don't give application full admin access\n-- Create specific user with minimal needed privileges\n\nCREATE USER 'webapp_user'@'%' IDENTIFIED BY 'strong_password';\nGRANT SELECT, INSERT, UPDATE ON UniversityDB.Students TO 'webapp_user'@'%';\nGRANT SELECT ON UniversityDB.Department TO 'webapp_user'@'%';\n-- No DELETE, DROP, or other dangerous operations\n\nUse Views for Security:\n-- Hide sensitive columns\nCREATE VIEW PublicStudentInfo AS\nSELECT StudentID, FirstName, LastName, DeptID\nFROM Students;\n-- SSN, GPA, Address hidden\n\nGRANT SELECT ON UniversityDB.PublicStudentInfo TO 'public_user'@'%';\n\n5. PERFORMANCE OPTIMIZATION:\n\nIndexing Strategy:\n  â€¢ Index columns used in WHERE clauses\n  â€¢ Index columns used in JOINs\n  â€¢ Index columns used in ORDER BY\n  â€¢ Don't over-index (slows INSERT/UPDATE)\n\n-- Good index candidates\nCREATE INDEX idx_dept ON Students(DeptID); -- JOIN column\nCREATE INDEX idx_gpa ON Students(GPA);     -- WHERE filter\nCREATE INDEX idx_name ON Students(LastName, FirstName); -- Composite for searches\n\nQuery Optimization:\n  â€¢ Use EXPLAIN to analyze query plans\n  â€¢ Avoid SELECT *; specify needed columns\n  â€¢ Use appropriate JOINs\n  â€¢ Consider query caching\n\nEXPLAIN SELECT S.FirstName, S.LastName, D.DeptName\nFROM Students S\nJOIN Department D ON S.DeptID = D.DeptID\nWHERE S.GPA > 3.5;\n\n6. INDUSTRY TOOLS:\n\n  â€¢ MySQL Workbench: Visual database design, query tool\n  â€¢ pgAdmin: PostgreSQL administration\n  â€¢ DBeaver: Universal database tool\n  â€¢ DataGrip: JetBrains database IDE\n  â€¢ phpMyAdmin: Web-based MySQL administration",
          exam: "IMPORTANT EXAM QUESTIONS:\n\n1. Explain the three-schema architecture of a DBMS with a neat diagram.\n   Answer should include: External, Conceptual, Internal levels with mappings\n\n2. What is data independence? Differentiate between logical and physical data independence with examples.\n   Key points: Definition, two types, examples of each, why important\n\n3. Distinguish between DDL, DML, and DCL with examples of each.\n   Create table showing: Purpose, commands, when used\n\n4. List and explain the components of a database system.\n   Include: Storage Manager, Query Processor, users, DBA\n\n5. What are the responsibilities of a Database Administrator (DBA)?\n   Points: Schema definition, access control, backup/recovery, performance tuning\n\n6. Explain the difference between procedural and non-procedural DML.\n   Examples from different database models\n\n7. Write DDL statements to create a database schema with at least 3 related tables including constraints.\n   Must include: PRIMARY KEY, FOREIGN KEY, CHECK, UNIQUE, NOT NULL\n\n8. Write SQL queries demonstrating all DML operations (SELECT, INSERT, UPDATE, DELETE).\n   Include: Simple and complex queries, joins, aggregation\n\n9. Explain client-server and three-tier database architectures.\n   Draw diagrams, explain components, when to use each\n\n10. How does the concept of data independence help in database maintenance and evolution?\n    Real-world scenarios\n\n11. What is the role of the Data Dictionary (System Catalog)?\n    What information does it store?\n\n12. Write DCL commands to grant and revoke privileges. Explain role-based access control.\n\nQUICK REVISION:\nâ€¢ External Level = User Views (WHAT users see)\nâ€¢ Conceptual Level = Logical Schema (WHAT is stored)\nâ€¢ Internal Level = Physical Storage (HOW stored)\nâ€¢ DDL = Define structure (CREATE, ALTER, DROP)\nâ€¢ DML = Manipulate data (SELECT, INSERT, UPDATE, DELETE)\nâ€¢ DCL = Control access (GRANT, REVOKE)\nâ€¢ TCL = Manage transactions (COMMIT, ROLLBACK)\nâ€¢ Physical Independence = Change storage without affecting logic\nâ€¢ Logical Independence = Change schema without affecting views\nâ€¢ DBA = Database Administrator (manages everything)",
          takeaways: "KEY TAKEAWAYS:\n\nâœ“ Three-Schema Architecture separates user views (external), logical structure (conceptual), and physical storage (internal)\n\nâœ“ Data independence allows changes at one level without affecting other levels - critical for system evolution\n\nâœ“ Physical data independence: Change storage details (indexes, file organization) without affecting applications\n\nâœ“ Logical data independence: Change logical schema without affecting user views (harder to achieve)\n\nâœ“ DDL defines database structure: CREATE, ALTER, DROP, TRUNCATE\n\nâœ“ DML manipulates data: SELECT (retrieve), INSERT (add), UPDATE (modify), DELETE (remove)\n\nâœ“ DCL controls access: GRANT (give permissions), REVOKE (remove permissions)\n\nâœ“ TCL manages transactions: COMMIT (save), ROLLBACK (undo), SAVEPOINT (intermediate points)\n\nâœ“ Non-procedural (declarative) DML like SQL is easier than procedural - you say WHAT, not HOW\n\nâœ“ Database system components: Storage Manager, Query Processor, DML Compiler, DDL Compiler\n\nâœ“ Three-tier architecture (Presentation, Application, Database) is standard for web applications\n\nâœ“ DBA responsibilities: Schema management, access control, backup/recovery, performance tuning, security\n\nâœ“ Data dictionary stores metadata: table definitions, constraints, indexes, privileges\n\nâœ“ Always use parameterized queries to prevent SQL injection attacks\n\nâœ“ Follow principle of least privilege: grant only necessary permissions\n\nREMEMBER: The layered architecture and data independence are what make modern databases flexible, maintainable, and able to evolve with changing business needs without requiring application rewrites!"
        }
      }
    ]
  },
  {
    id: "unit-2",
    title: "UNIT II: Data Models & Query Languages",
    topics: [
      {
        id: "data-models",
        title: "Data Models",
        subtopics: ["Entity-relationship model", "Network model", "Relational and Object oriented data Models", "Integrity constraints", "Data manipulation operations"],
        clos: ["CLO03"],
        cos: ["CO01"],
        content: {
          introduction: "Data models are abstract representations that define how data is structured, stored, and manipulated in a database. They serve as the blueprint for database design, helping us understand relationships between different data elements and enforcing rules that maintain data quality.",
          concept: "A data model is a conceptual representation of data objects, their associations, and the rules governing operations on them. The Entity-Relationship (ER) model is the most popular conceptual model for database design.\n\nKey Components:\n\n1. Entities: Real-world objects (Student, Course, Department)\n2. Attributes: Properties of entities (StudentID, Name, Age)\n3. Relationships: Associations between entities (Student ENROLLS Course)\n4. Cardinality: One-to-One (1:1), One-to-Many (1:N), Many-to-Many (M:N)\n\nThe ER model helps visualize database structure before implementation. It captures business requirements in a form that can be translated into relational tables.\n\nObject-Oriented Data Models extend this by supporting inheritance, encapsulation, and complex data typesâ€”useful for multimedia and CAD applications.",
          technicalDepth: "Entity-Relationship Model Components:\n\n1. Strong Entity: Has its own primary key (e.g., Student with StudentID)\n2. Weak Entity: Depends on strong entity (e.g., Dependent relies on Employee)\n3. Attributes Types:\n   - Simple vs Composite (Name vs {FirstName, LastName})\n   - Single-valued vs Multi-valued (Age vs PhoneNumbers)\n   - Stored vs Derived (DateOfBirth vs Age)\n\nRelationship Types:\n- Unary (recursive): Employee MANAGES Employee\n- Binary: Student ENROLLS Course\n- Ternary: Professor TEACHES Course IN Semester\n\nIntegrity Constraints:\n\n1. Domain Constraint: Values must be from specified domain\n   Example: Age must be integer between 0-120\n\n2. Entity Integrity: Primary key cannot be NULL\n   Why? Need unique identification for each tuple\n\n3. Referential Integrity: Foreign key must match primary key in referenced table\n   Example: StudentID in Enrollment must exist in Student table\n\n4. Key Constraints:\n   - Superkey: Any combination that uniquely identifies\n   - Candidate Key: Minimal superkey\n   - Primary Key: Chosen candidate key\n   - Alternate Key: Other candidate keys\n\nMapping ER to Relational:\n- Each entity becomes a table\n- Attributes become columns\n- Relationships handled via foreign keys or junction tables (for M:N)",
          examples: "ER Diagram Example - University Database:\n\nEntities:\nStudent(StudentID*, Name, Email, Major)\nCourse(CourseID*, CourseName, Credits)\nProfessor(ProfID*, ProfName, Department)\n\nRelationships:\n1. Student ENROLLS Course (M:N)\n   Junction Table: Enrollment(StudentID, CourseID, Grade, Semester)\n\n2. Professor TEACHES Course (1:N)\n   Foreign Key: Course.ProfID references Professor.ProfID\n\nConstraints in SQL:\n\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    Name VARCHAR(100) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    Age INT CHECK (Age >= 16 AND Age <= 100)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID INT,\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID),\n    FOREIGN KEY (StudentID) REFERENCES Student(StudentID)\n        ON DELETE CASCADE,\n    FOREIGN KEY (CourseID) REFERENCES Course(CourseID)\n        ON UPDATE CASCADE\n);\n\nReal-World Example - E-Commerce:\n- Customer HAS Address (1:N - composite attribute becomes table)\n- Order CONTAINS Product (M:N - junction table OrderItem)\n- Product BELONGS_TO Category (N:1 - foreign key in Product)",
          practical: "ER modeling is crucial in system design. Before writing any code, database architects create ER diagrams to:\n\n1. Communicate with stakeholders using visual representations\n2. Identify data requirements and relationships\n3. Prevent data redundancy and anomalies\n4. Plan for scalability and future changes\n\nReal applications:\n- E-commerce platforms model customers, orders, products\n- Hospital systems model patients, doctors, appointments, prescriptions\n- Banking systems model accounts, transactions, customers, loans\n\nTools: MySQL Workbench, Lucidchart, draw.io for ER diagram creation.",
          exam: "Important Questions:\n\n1. Draw ER diagram for library management system with entities: Book, Member, Author, Publisher\n2. Differentiate between strong and weak entities with examples\n3. Explain all types of integrity constraints with SQL examples\n4. Convert given ER diagram to relational schema\n5. What is the difference between candidate key and primary key?\n6. How do you handle M:N relationships in relational model?\n7. Define cardinality and participation constraints in relationships",
          takeaways: "Key Points:\n\nâ€¢ Data models bridge the gap between real-world requirements and database implementation\nâ€¢ ER model is the standard for conceptual database design\nâ€¢ Integrity constraints ensure data accuracy and consistency\nâ€¢ Proper modeling prevents data anomalies and redundancy\nâ€¢ Understanding cardinality is crucial for correct table design\nâ€¢ Foreign keys implement relationships in relational databases"
        }
      },
      {
        id: "relational-query-languages",
        title: "Relational Query Languages",
        subtopics: ["Relational algebra", "Tuple and domain relational calculus", "Open source and commercial DBMS"],
        clos: ["CLO04"],
        cos: ["CO02"],
        content: {
          introduction: "Query languages are formal languages used to retrieve and manipulate data in databases. They form the backbone of database interaction, allowing users to express complex data requirements in a structured way. Relational algebra provides the mathematical foundation for all query languages, including SQL.",
          concept: "Query languages come in two flavors:\n\n1. Procedural (Relational Algebra): You specify WHAT data you want and HOW to get it\n2. Non-procedural (Relational Calculus, SQL): You specify WHAT data you want, system figures out HOW\n\nRelational Algebra is a formal language with a set of operations for manipulating relations (tables). It's the theoretical basis for SQL and database query optimization.\n\nCore Principles:\n- Input: One or more relations\n- Output: A new relation\n- Operations can be composed (output of one is input to another)\n- Closure property: Result is also a relation\n\nWhy Learn Relational Algebra?\n- Understand query optimization\n- Foundation for SQL\n- Helps in query formulation\n- Used in query execution plans",
          technicalDepth: "Relational Algebra Operations:\n\n1. UNARY OPERATIONS (single relation):\n\nSelect (Ïƒ): Filters rows based on condition\n  Notation: Ïƒ<condition>(Relation)\n  Example: Ïƒ(Age>21)(Student) â€” students older than 21\n  Properties: Ïƒ(c1 AND c2)(R) = Ïƒ(c1)(Ïƒ(c2)(R)) â€” commutative\n\nProject (Ï€): Selects specific columns\n  Notation: Ï€<attributes>(Relation)\n  Example: Ï€(Name, Major)(Student) â€” only names and majors\n  Note: Removes duplicates automatically\n\nRename (Ï): Changes relation or attribute names\n  Notation: Ï(NewName, Relation)\n  Useful for self-joins and clarity\n\n2. BINARY OPERATIONS (two relations):\n\nUnion (âˆª): Combines tuples from both relations\n  Requirement: Relations must be union-compatible (same schema)\n  R âˆª S: All tuples in R or S (no duplicates)\n\nIntersection (âˆ©): Common tuples\n  R âˆ© S: Tuples present in both R and S\n\nSet Difference (âˆ’): Tuples in first but not second\n  R âˆ’ S: Tuples in R but not in S\n  Note: NOT commutative (R âˆ’ S â‰  S âˆ’ R)\n\nCartesian Product (Ã—): All possible combinations\n  R Ã— S: Each tuple in R paired with each in S\n  Result has |R| Ã— |S| tuples\n  Columns: All columns from R and S\n\n3. JOIN OPERATIONS (most important):\n\nNatural Join (â‹ˆ): Combines relations based on common attributes\n  Employee â‹ˆ Department: Matches on common DeptID\n  Eliminates duplicate columns\n\nTheta Join (â‹ˆÎ¸): Join with condition\n  R â‹ˆ(R.a=S.b) S\n  Generalized form of join\n\nEqui Join: Theta join with equality condition\n\nOuter Joins (preserve unmatched tuples):\n  - Left Outer Join (âŸ•): All tuples from left + matched from right\n  - Right Outer Join (âŸ–): All tuples from right + matched from left\n  - Full Outer Join (âŸ—): All tuples from both\n\nSemi Join (â‹‰): Only attributes from left relation\n  R â‹‰ S = Ï€(R attributes)(R â‹ˆ S)\n\n4. AGGREGATE OPERATIONS:\n\nGrouping (Î³): Group by attributes and apply aggregate functions\n  Î³(Dept, COUNT(EmpID))(Employee)\n  Functions: COUNT, SUM, AVG, MIN, MAX\n\nRelational Calculus:\n\nTuple Relational Calculus (TRC):\n  { t | P(t) } â€” set of tuples t for which predicate P is true\n  Example: { t | t âˆˆ Employee âˆ§ t.Salary > 50000 }\n\nDomain Relational Calculus (DRC):\n  Uses domain variables instead of tuple variables\n  Example: { <n, s> | âˆƒd (Employee(n, d, s) âˆ§ s > 50000) }\n\nSQL (Structured Query Language):\nBased on tuple relational calculus but with procedural elements.\nCommercial DBMS: MySQL, PostgreSQL, Oracle, SQL Server, MongoDB",
          examples: "Complex Query Examples:\n\nScenario: University Database\nStudent(SID, Name, Major, Age)\nEnrollment(SID, CID, Grade)\nCourse(CID, CourseName, Credits)\n\n1. Find names of students enrolled in 'DBMS'\n\nRelational Algebra:\nÏ€(Name)(Ïƒ(CourseName='DBMS')(Student â‹ˆ Enrollment â‹ˆ Course))\n\nSQL:\nSELECT DISTINCT S.Name\nFROM Student S\nJOIN Enrollment E ON S.SID = E.SID\nJOIN Course C ON E.CID = C.CID\nWHERE C.CourseName = 'DBMS';\n\n2. Students NOT enrolled in any course\n\nRelational Algebra:\nÏ€(SID)(Student) âˆ’ Ï€(SID)(Enrollment)\n\nSQL:\nSELECT SID FROM Student\nEXCEPT\nSELECT SID FROM Enrollment;\n\n3. Students enrolled in ALL courses\n\nRelational Algebra (Division):\nÏ€(SID, CID)(Enrollment) Ã· Ï€(CID)(Course)\n\nSQL:\nSELECT S.SID\nFROM Student S\nWHERE NOT EXISTS (\n    SELECT C.CID FROM Course C\n    WHERE NOT EXISTS (\n        SELECT * FROM Enrollment E\n        WHERE E.SID = S.SID AND E.CID = C.CID\n    )\n);\n\n4. Average grade per student\n\nRelational Algebra:\nÎ³(SID, AVG(Grade))(Enrollment)\n\nSQL:\nSELECT SID, AVG(Grade) as AvgGrade\nFROM Enrollment\nGROUP BY SID;\n\n5. Self-Join: Find employees earning more than their manager\n\nSQL:\nSELECT E.Name\nFROM Employee E, Employee M\nWHERE E.ManagerID = M.EmpID\nAND E.Salary > M.Salary;\n\nOpen Source DBMS: MySQL, PostgreSQL, SQLite, MariaDB\nCommercial DBMS: Oracle, Microsoft SQL Server, IBM DB2",
          practical: "Query languages are used everywhere:\n\n1. Web Applications: Fetch user data, process transactions\n2. Analytics: Business intelligence, reporting dashboards\n3. Data Science: Extract data for machine learning models\n4. APIs: Backend services query databases for client requests\n\nPerformance Considerations:\n- Query optimization: DBMS converts SQL to efficient relational algebra\n- Indexing: Speeds up SELECT operations\n- Join order matters: Employee â‹ˆ Dept vs Dept â‹ˆ Employee\n- Understanding algebra helps write efficient queries\n\nReal-world SQL Usage:\n- Complex JOINs for related data (orders + customers + products)\n- Aggregations for reports (total sales per region)\n- Subqueries for filtering (students with above-average grades)\n- Transactions for consistency (bank transfers)",
          exam: "Expected Questions:\n\n1. Write relational algebra for: Find students enrolled in more than 3 courses\n2. Convert given SQL query to relational algebra expression\n3. Explain all types of JOIN operations with examples\n4. Differentiate between relational algebra and relational calculus\n5. What is division operation? When is it used?\n6. Write SQL query using GROUP BY and HAVING\n7. Explain query optimization with relational algebra\n8. Compare open-source vs commercial DBMS (features, licensing, performance)",
          takeaways: "Essential Concepts:\n\nâ€¢ Relational algebra is the foundation of all query languages\nâ€¢ SQL is based on relational calculus but includes procedural elements\nâ€¢ Understanding algebra helps in query optimization\nâ€¢ Join operations are the most powerful and commonly used\nâ€¢ Set operations require union-compatible relations\nâ€¢ Query execution plans use relational algebra internally\nâ€¢ Choosing the right DBMS depends on requirements: scale, budget, features"
        }
      }
    ]
  },
  {
    id: "unit-3",
    title: "UNIT III: Relational Database Design",
    topics: [
      {
        id: "relational-design",
        title: "Relational Database Design",
        subtopics: ["Domain and data dependency", "Armstrong's axioms", "Functional dependencies", "Normal forms", "Dependency preservation", "Lossless design"],
        clos: ["CLO03"],
        cos: ["CO03"],
        content: {
          introduction: "Database design is the art and science of organizing data to minimize redundancy while ensuring integrity. Poor design leads to data anomaliesâ€”inconsistencies that creep in during updates, deletions, or insertions. Normalization is the systematic process that transforms poorly designed schemas into well-structured ones.",
          concept: "Normalization decomposes relations with anomalies into smaller, well-structured relations. The goal: eliminate redundancy without losing information.\n\nWhy Normalize?\n\n1. Update Anomaly: Changing data requires multiple updates\n   Example: If professor phone is stored with each course they teach, updating phone requires changing multiple rows\n\n2. Deletion Anomaly: Deleting one fact loses other facts\n   Example: Deleting last student in a course might delete course information\n\n3. Insertion Anomaly: Cannot insert some facts without others\n   Example: Cannot add new course without enrolling a student\n\nFunctional Dependencies:\nCore concept in normalization. X â†’ Y means X determines Y.\n\nExample: StudentID â†’ Name (StudentID determines Name)\nGiven StudentID, we can find exactly one Name.\n\nTypes:\n- Trivial FD: Y âŠ† X (Age, Name) â†’ Age\n- Non-trivial: Y âŠˆ X\n- Full FD: No proper subset of X determines Y\n- Partial FD: Some proper subset of X determines Y\n- Transitive FD: X â†’ Y, Y â†’ Z, then X â†’ Z",
          technicalDepth: "Normal Forms (Progressive Refinement):\n\n0NF (Unnormalized): Repeating groups, nested structures\nExample: Student(ID, Name, Courses{CourseID, Grade})\n\n1NF (First Normal Form):\nRule: All attributes must contain atomic (indivisible) values\nNo repeating groups or arrays\n\nBefore: Student(ID, Name, Phones)\n  Phones = '123-4567, 987-6543'\nAfter: Student(ID, Name, Phone)\n  Two rows if two phones\n\n2NF (Second Normal Form):\nRule: 1NF + No partial dependencies\nApplies only when primary key is composite\n\nExample Problem:\nEnrollment(StudentID, CourseID, StudentName, Grade)\nPK: (StudentID, CourseID)\nIssue: StudentID â†’ StudentName (partial dependency)\n\nSolution:\nStudent(StudentID, StudentName)\nEnrollment(StudentID, CourseID, Grade)\n\n3NF (Third Normal Form):\nRule: 2NF + No transitive dependencies\nNo non-key attribute depends on another non-key attribute\n\nExample Problem:\nEmployee(EmpID, EmpName, DeptID, DeptName)\nEmpID â†’ DeptID, DeptID â†’ DeptName (transitive)\n\nSolution:\nEmployee(EmpID, EmpName, DeptID)\nDepartment(DeptID, DeptName)\n\nBCNF (Boyce-Codd Normal Form):\nRule: For every non-trivial FD X â†’ Y, X must be a superkey\nStricter than 3NF\n\nExample Problem:\nCourseSchedule(Student, Course, Instructor)\nAssumptions:\n- Each course has multiple sections\n- Each instructor teaches one section\n- Student can take only one section per course\n\nFDs: {Student, Course} â†’ Instructor\n     Instructor â†’ Course\nIssue: Instructor â†’ Course violates BCNF (Instructor not superkey)\n\nSolution:\nInstructorCourse(Instructor, Course)\nStudentInstructor(Student, Instructor)\n\nArmstrong's Axioms (Inference Rules):\n\n1. Reflexivity: If Y âŠ† X, then X â†’ Y\n2. Augmentation: If X â†’ Y, then XZ â†’ YZ\n3. Transitivity: If X â†’ Y and Y â†’ Z, then X â†’ Z\n\nDerived Rules:\n4. Union: If X â†’ Y and X â†’ Z, then X â†’ YZ\n5. Decomposition: If X â†’ YZ, then X â†’ Y and X â†’ Z\n6. Pseudotransitivity: If X â†’ Y and WY â†’ Z, then WX â†’ Z\n\nClosure of Attributes:\nFind all attributes determined by a set of attributes.\nUsed to find candidate keys.\n\nAlgorithm:\n1. Start with given attributes\n2. Apply FDs to add more attributes\n3. Repeat until no new attributes\n\nDecomposition Properties:\n\n1. Lossless Join: Can reconstruct original relation\n   Test: R1 â‹ˆ R2 = R\n\n2. Dependency Preservation: All FDs can be checked without joins\n   Important for efficiency",
          examples: "Step-by-Step Normalization:\n\nOriginal Relation (0NF):\nStudentCourse(StudentID, StudentName, Address, CourseID, \n              CourseName, InstructorID, InstructorName, Grade)\n\nSample Data:\n101, John, NYC, C1, DBMS, I1, Smith, A\n101, John, NYC, C2, OS, I2, Jones, B\n102, Mary, LA, C1, DBMS, I1, Smith, A\n\nProblems: Redundancy (John's info repeated), Update anomalies\n\nStep 1: 1NF (Already atomic)\n\nStep 2: 2NF\nIdentify FDs:\nStudentID â†’ StudentName, Address\nCourseID â†’ CourseName, InstructorID, InstructorName\n{StudentID, CourseID} â†’ Grade\n\nDecompose:\nStudent(StudentID, StudentName, Address)\nCourse(CourseID, CourseName, InstructorID, InstructorName)\nEnrollment(StudentID, CourseID, Grade)\n\nStep 3: 3NF\nIn Course table: InstructorID â†’ InstructorName (transitive)\n\nFinal Schema:\nStudent(StudentID, StudentName, Address)\nInstructor(InstructorID, InstructorName)\nCourse(CourseID, CourseName, InstructorID)\nEnrollment(StudentID, CourseID, Grade)\n\nSQL Implementation:\n\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    StudentName VARCHAR(100),\n    Address VARCHAR(200)\n);\n\nCREATE TABLE Instructor (\n    InstructorID INT PRIMARY KEY,\n    InstructorName VARCHAR(100)\n);\n\nCREATE TABLE Course (\n    CourseID VARCHAR(10) PRIMARY KEY,\n    CourseName VARCHAR(100),\n    InstructorID INT,\n    FOREIGN KEY (InstructorID) REFERENCES Instructor(InstructorID)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID VARCHAR(10),\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID),\n    FOREIGN KEY (StudentID) REFERENCES Student(StudentID),\n    FOREIGN KEY (CourseID) REFERENCES Course(CourseID)\n);",
          practical: "Real-World Considerations:\n\n1. Over-Normalization:\n   Too many tables can hurt performance (many joins)\n   Solution: Denormalize strategically for read-heavy systems\n\n2. When to Denormalize:\n   - Read performance critical (analytics, reporting)\n   - Acceptable redundancy (derived/cached data)\n   - NoSQL databases (MongoDB stores nested documents)\n\n3. Industry Practice:\n   - OLTP systems: Normalize to 3NF/BCNF\n   - Data warehouses: Denormalize (star schema, snowflake schema)\n   - E-commerce: Balance between normalized inventory and denormalized product displays\n\n4. Modern Approaches:\n   - Microservices: Each service has its own optimized schema\n   - Event sourcing: Store events, build projections\n   - CQRS: Separate read and write models",
          exam: "Important Exam Questions:\n\n1. Given relation R(A,B,C,D,E) with FDs: Aâ†’B, BCâ†’E, EDâ†’A\n   Find: All candidate keys, Highest normal form\n\n2. Normalize given unnormalized relation to BCNF\n\n3. Explain with examples: Update, Delete, Insert anomalies\n\n4. Difference between 3NF and BCNF with example where 3NF â‰  BCNF\n\n5. Prove Armstrong's Axioms are sound and complete\n\n6. Given FD set, find minimal cover (canonical cover)\n\n7. Check if decomposition is lossless and dependency-preserving\n\n8. When would you denormalize a database? Trade-offs?",
          takeaways: "Core Principles:\n\nâ€¢ Normalization eliminates redundancy and prevents anomalies\nâ€¢ Each normal form builds on previous ones (1NF â†’ 2NF â†’ 3NF â†’ BCNF)\nâ€¢ Functional dependencies are key to understanding normalization\nâ€¢ BCNF is the ideal, but 3NF is often sufficient in practice\nâ€¢ Decomposition should be lossless and preserve dependencies\nâ€¢ Over-normalization can hurt read performance\nâ€¢ Real systems balance normalization with performance needs"
        }
      },
      {
        id: "query-processing",
        title: "Query Processing and Optimization",
        subtopics: ["Evaluation of relational algebra expressions", "Query equivalence", "Join strategies", "Query optimization algorithms"],
        clos: ["CLO04"],
        cos: ["CO04"],
        content: {
          introduction: "When you write a SQL query, the database doesn't just blindly execute it. Behind the scenes, the query processor analyzes, transforms, and optimizes your query to find the fastest way to get results. Understanding this process helps you write better queries and diagnose performance issues.",
          concept: "Query processing is the set of activities performed by the DBMS to translate a high-level query (SQL) into an efficient execution plan.\n\nPhases of Query Processing:\n\n1. Parsing & Translation:\n   - Check syntax\n   - Verify table/column names exist\n   - Translate SQL to relational algebra\n   - Build query tree\n\n2. Optimization:\n   - Generate multiple execution plans\n   - Estimate cost of each plan\n   - Choose the best plan\n\n3. Evaluation:\n   - Execute the chosen plan\n   - Return results\n\nWhy Optimization Matters:\nConsider: SELECT * FROM Student JOIN Enrollment WHERE Grade='A'\n\nBad Plan: Join all students with all enrollments, then filter\nGood Plan: Filter first (ÏƒGrade='A'), then join smaller result\n\nFor large tables (millions of rows), the difference can be seconds vs hours.",
          technicalDepth: "Query Optimization Approaches:\n\n1. HEURISTIC OPTIMIZATION (Rule-Based):\n\nRules that generally improve performance:\n\nRule 1: Perform selections early\n  Ï€name(Ïƒsalary>50000(Employee â‹ˆ Department))\n  Better: Ï€name(Ïƒsalary>50000(Employee) â‹ˆ Department)\n  Why? Smaller intermediate result\n\nRule 2: Perform projections early\n  Reduce tuple size, fewer I/Os\n\nRule 3: Combine selections\n  Ïƒc1(Ïƒc2(R)) = Ïƒ(c1 AND c2)(R)\n  Single scan instead of two\n\nRule 4: Combine projections\n  Similar to selections\n\nRule 5: Break complex selections with OR\n  Ïƒ(c1 OR c2)(R) can use union of two index scans\n\nRule 6: Evaluate Cartesian products with joins\n  Ïƒcondition(R Ã— S) = R â‹ˆcondition S\n  Join algorithms are optimized\n\n2. COST-BASED OPTIMIZATION:\n\nEstimate cost of each plan using:\n\nA. Statistics:\n   - Number of tuples (n)\n   - Tuple size (bytes)\n   - Number of distinct values (V)\n   - Index structure (B-tree depth, hash buckets)\n\nB. Cost Components:\n   - Disk I/O cost (dominant factor)\n   - CPU cost (comparisons, sorting)\n   - Memory usage\n   - Network cost (distributed databases)\n\nC. Selectivity Estimation:\n   Selectivity = fraction of tuples satisfying condition\n\n   For A = value: 1/V(A) (uniform distribution assumption)\n   For A > value: (max - value)/(max - min)\n   For A AND B: s(A) Ã— s(B) (independence assumption)\n   For A OR B: s(A) + s(B) - s(A)Ã—s(B)\n\nJOIN STRATEGIES:\n\n1. Nested Loop Join:\n   For each tuple r in R:\n     For each tuple s in S:\n       If join-condition(r,s): output\n   Cost: n(R) + n(R)Ã—n(S)\n   Best when: One relation is small\n\n2. Block Nested Loop:\n   Process blocks instead of tuples\n   Cost: b(R) + b(R)Ã—b(S)\n   Better I/O performance\n\n3. Index Nested Loop:\n   For each tuple r in R:\n     Use index on S to find matching tuples\n   Cost: n(R) + n(R)Ã—c\n   Where c = cost of index lookup\n   Best when: Index exists on join attribute\n\n4. Sort-Merge Join:\n   Sort both relations on join attribute\n   Merge sorted relations\n   Cost: b(R) + b(S) + sort cost\n   Best for: Large sorted or nearly-sorted relations\n\n5. Hash Join:\n   Build phase: Hash smaller relation\n   Probe phase: Hash larger relation, probe hash table\n   Cost: 3(b(R) + b(S))\n   Best for: Equi-joins on large unsorted relations\n   Most commonly used in practice\n\nQuery Equivalence:\nTwo queries are equivalent if they produce same result for any database state.\n\nEquivalence Rules:\n1. Ïƒ(c1 AND c2)(R) = Ïƒc1(Ïƒc2(R))\n2. Ïƒc1(Ïƒc2(R)) = Ïƒc2(Ïƒc1(R))\n3. Ï€L1(Ï€L2(...(Ï€Ln(R)))) = Ï€L1(R)\n4. Ïƒc(R Ã— S) = R â‹ˆc S\n5. R â‹ˆ S = S â‹ˆ R (commutative)\n6. (R â‹ˆ S) â‹ˆ T = R â‹ˆ (S â‹ˆ T) (associative)\n\nDynamic Programming for Join Ordering:\nFor n tables, there are (2n-2)!/(n-1)! possible join orders.\nDP finds optimal order in O(nÂ· 2^n) time.",
          examples: "Example 1: Query Transformation\n\nOriginal SQL:\nSELECT name\nFROM Employee, Department\nWHERE Employee.deptId = Department.id\n  AND Department.location = 'NYC'\n  AND Employee.salary > 50000;\n\nNaive Plan:\n1. Employee Ã— Department (huge Cartesian product)\n2. Filter all conditions\n3. Project name\n\nOptimized Plan:\n1. Ïƒlocation='NYC'(Department) â€” filter departments first\n2. Ïƒsalary>50000(Employee) â€” filter employees first\n3. Filtered_Employee â‹ˆ Filtered_Department â€” smaller join\n4. Ï€name â€” project at end\n\nExample 2: Join Strategy Selection\n\nQuery: Employee â‹ˆdeptId Department\n\nScenario A: Employee (1M rows), Department (50 rows)\nBest: Index Nested Loop\n  - Build index on Employee.deptId\n  - For each department, lookup matching employees\n  - Cost: 50 index lookups\n\nScenario B: Employee (1M rows), Department (500K rows)\nBest: Hash Join\n  - Hash smaller relation (Department)\n  - Probe with Employee\n  - Single pass through both relations\n\nExample 3: Cost Estimation\n\nGiven:\n- Employee: 10,000 tuples, 100 tuples/block = 100 blocks\n- Department: 50 tuples, 5 tuples/block = 10 blocks\n- Join: Employee.deptId = Department.id\n\nNested Loop:\n  Cost = 100 + 10,000 Ã— 10 = 100,100 block accesses\n\nBlock Nested Loop:\n  Cost = 100 + 100 Ã— 10 = 1,100 block accesses\n\nHash Join:\n  Cost = 3(100 + 10) = 330 block accesses\n\nWinner: Hash Join\n\nEXPLAIN Command:\n\nEXPLAIN SELECT * FROM Employee WHERE salary > 50000;\n\nOutput shows:\n- Scan type (Sequential, Index, Bitmap)\n- Rows estimate\n- Cost estimate\n- Filter conditions\n- Join method\n\nExample Output:\nSeq Scan on employee (cost=0.00..180.00 rows=1000 width=50)\n  Filter: (salary > 50000)\n\nVs with index:\nIndex Scan using idx_salary (cost=0.00..45.00 rows=1000 width=50)\n  Index Cond: (salary > 50000)",
          practical: "Performance Tuning Tips:\n\n1. Write Efficient Queries:\n   - Use WHERE to filter early\n   - Avoid SELECT *, specify needed columns\n   - Use LIMIT when possible\n   - Avoid functions on indexed columns\n\n2. Indexing Strategy:\n   - Index foreign keys\n   - Index columns in WHERE, JOIN, ORDER BY\n   - Avoid over-indexing (slows INSERT/UPDATE)\n   - Consider composite indexes for multiple columns\n\n3. Understand Query Plans:\n   - Use EXPLAIN to see execution plan\n   - Look for sequential scans on large tables\n   - Check join methods\n   - Monitor rows estimates vs actual\n\n4. Database Configuration:\n   - Buffer pool size (memory for caching)\n   - Work_mem (memory for sorting, hashing)\n   - Statistics collection (ANALYZE command)\n\n5. Query Hints (use sparingly):\n   - Force index usage\n   - Specify join order\n   - Enable/disable certain optimizations\n\nReal-World Scenarios:\n- E-commerce: Optimize product search queries\n- Analytics: Optimize complex aggregation queries\n- Social media: Optimize feed generation queries\n- Financial: Optimize transaction history queries",
          exam: "Expected Questions:\n\n1. Explain phases of query processing with diagram\n2. What is the difference between heuristic and cost-based optimization?\n3. Compare all join strategies: nested loop, block nested loop, index nested loop, sort-merge, hash\n4. Given query and statistics, calculate cost for different join orders\n5. Transform given SQL query to optimized relational algebra\n6. Explain dynamic programming approach for join ordering\n7. What is selectivity? How is it estimated?\n8. Given EXPLAIN output, identify performance bottlenecks\n9. Why is pushing selections down beneficial?\n10. Explain the role of indexes in query optimization",
          takeaways: "Key Concepts:\n\nâ€¢ Query optimization chooses the fastest execution plan from many alternatives\nâ€¢ Heuristic rules provide quick optimizations (push selections down)\nâ€¢ Cost-based optimization uses statistics to estimate plan costs\nâ€¢ Join strategy selection depends on table sizes and available indexes\nâ€¢ Hash join is usually best for large equi-joins\nâ€¢ Index nested loop is best when one table is small\nâ€¢ Understanding optimization helps write efficient queries\nâ€¢ EXPLAIN command reveals execution plans for tuning"
        }
      }
    ]
  },
  {
    id: "unit-4",
    title: "UNIT IV: Storage & Transactions",
    topics: [
      {
        id: "storage-strategies",
        title: "Storage Strategies",
        subtopics: ["Indices", "B-trees", "Hashing"],
        clos: ["CLO02"],
        cos: ["CO03"],
        content: {
          introduction: "The gap between memory speed and disk speed is vastâ€”memory access takes nanoseconds, disk access takes milliseconds. That's a million-fold difference. Storage strategies bridge this gap through clever data structures, caching, and organization techniques that minimize disk I/O, the primary bottleneck in database performance.",
          concept: "Storage strategies address the physical organization of data on disk and in memory to optimize access patterns.\n\nStorage Hierarchy:\n\n1. Primary Storage (RAM):\n   - Fast (nanoseconds)\n   - Volatile (lost on power off)\n   - Expensive\n   - Limited capacity\n\n2. Secondary Storage (HDD/SSD):\n   - Slower (milliseconds for HDD, microseconds for SSD)\n   - Persistent\n   - Cheaper\n   - Large capacity\n\n3. Tertiary Storage (Tape, Cloud):\n   - Slowest (seconds)\n   - Archival purposes\n\nKey Principle: Minimize disk I/O by:\n- Keeping frequently accessed data in memory (buffer pool)\n- Using indexes to avoid full table scans\n- Storing related data together (clustering)\n- Compressing data to reduce I/O volume",
          technicalDepth: "FILE ORGANIZATION:\n\n1. Heap File (Unordered):\n   Records stored in insertion order\n   Insert: O(1) - append at end\n   Search: O(n) - scan entire file\n   Delete: O(n) - find then mark deleted\n   Best for: Bulk loading, full table scans\n\n2. Sorted File:\n   Records sorted by search key\n   Insert: O(n) - maintain order\n   Search: O(log n) - binary search\n   Range queries: Efficient\n   Best for: Static data with range queries\n\n3. Hash File:\n   Records distributed across buckets using hash function\n   Search: O(1) average - compute hash, go to bucket\n   Range queries: Inefficient (requires full scan)\n   Best for: Exact match lookups\n\nINDEXING STRUCTURES:\n\n1. B-TREE:\n\nProperties:\n- Balanced tree (all leaves at same level)\n- Order d: Each node has [d, 2d] keys (except root)\n- Internal nodes: [d+1, 2d+1] children\n- Keeps data sorted\n\nStructure:\nNode: [P1, K1, P2, K2, ..., Pn, Kn, Pn+1]\n- Ki = keys (sorted)\n- Pi = pointers to children or data\n- All keys in subtree Pi < Ki < all keys in subtree Pi+1\n\nSearch Algorithm:\n1. Start at root\n2. Binary search keys in node\n3. Follow appropriate pointer\n4. Repeat until leaf\n\nInsertion:\n1. Search to find leaf\n2. Insert key in sorted order\n3. If leaf overflows (> 2d keys):\n   - Split into two nodes\n   - Promote middle key to parent\n   - Recursively split parent if needed\n\nDeletion:\n1. Search and remove key\n2. If underflow (< d keys):\n   - Borrow from sibling, OR\n   - Merge with sibling\n   - Recursively fix parent\n\nCost Analysis (n records, order d):\n- Tree height: log_d(n)\n- Search: O(log n) disk I/Os\n- Insert/Delete: O(log n) disk I/Os\n\n2. B+ TREE (Most Common in Databases):\n\nDifferences from B-Tree:\n1. All data in leaf nodes\n2. Internal nodes only store keys + pointers\n3. Leaf nodes linked (sequential access)\n\nAdvantages:\n- Higher fanout (more keys per internal node)\n- Shorter tree (fewer I/Os)\n- Efficient range scans (follow leaf pointers)\n- Sequential access without tree traversal\n\nExample B+ Tree (order d=2):\n                [30]\n               /    \\\n          [10,20]  [40,50]\n         /  |  \\    /  |  \\\nLeaves: [5,7][12,15][25,28][35,38][45,48][55,60]\nLeaves linked: â†’ allows sequential scan\n\n3. HASH INDEX:\n\nStatic Hashing:\nhash(key) % M = bucket number\nIssue: Fixed M, poor with growing data\n\nDynamic Hashing (Extendible Hashing):\n- Bucket directory grows/shrinks\n- Only affected buckets split\n- Efficient for insertions\n\nLinear Hashing:\n- No directory\n- Gradual bucket splitting\n- Handles overflows with chaining\n\nBUFFER MANAGEMENT:\n\nBuffer Pool: Area of memory for caching disk pages\n\nReplacement Policies:\n\n1. LRU (Least Recently Used):\n   Replace page not used for longest time\n   Good general-purpose policy\n\n2. MRU (Most Recently Used):\n   Replace most recently used\n   Good for sequential scans\n\n3. Clock Algorithm:\n   Approximates LRU with less overhead\n   Each page has reference bit\n\n4. LRU-K:\n   Track K most recent accesses\n   Better for database workloads\n\nPin Count: Number of active users of a page\n- Pinned pages cannot be replaced\n- Must unpin after use\n\nDirty Bit: Indicates modified page\n- Must write back before replacement\n- WAL rule: Log before data\n\nCLUSTERING:\n\nStore related records physically close\n\nTypes:\n1. Intra-file clustering: Related records in same file\n2. Inter-file clustering: Records from multiple tables together\n\nClustered Index:\n- Data stored in index order\n- Only one per table\n- Improves range queries\n- Slows insertions (maintain order)\n\nExample: Customer table clustered by CustomerID\n- CustomerID 1-1000 in block 1\n- CustomerID 1001-2000 in block 2\n- Range query efficient",
          examples: "Example 1: B+ Tree Operations\n\nInitial B+ Tree (order 3, max 5 keys per node):\n                     [30]\n                   /      \\\n             [10,20]      [40,50]\n            /   |   \\      /  |  \\\nLeaves: [5,7,9][12,15,18][25,27,29][35,38][45,48][55,60]\n\nInsert 13:\n1. Find correct leaf: [12,15,18]\n2. Insert: [12,13,15,18]\n3. No overflow, done\n\nInsert 14:\n1. Find leaf: [12,13,15,18]\n2. Insert: [12,13,14,15,18]\n3. No overflow, done\n\nInsert 16:\n1. Find leaf: [12,13,14,15,18]\n2. Insert: [12,13,14,15,16,18]\n3. Overflow! Split:\n   Left: [12,13,14]\n   Right: [15,16,18]\n   Promote 15 to parent: [10,15,20]\n\nExample 2: Index Selection\n\nQuery: SELECT * FROM Orders WHERE OrderDate BETWEEN '2024-01-01' AND '2024-01-31';\n\nOption A: No index\n- Full table scan: 1M orders Ã— 100 bytes = 100MB\n- Assume 4KB pages: 25,000 page reads\n\nOption B: B+ Tree index on OrderDate\n- Assume 5% orders in range = 50K orders\n- Tree height: 3 levels\n- Cost: 3 (tree traversal) + 50K/100 (data pages) = 503 page reads\n- 50x faster!\n\nExample 3: Hash vs B+ Tree\n\nExact Match Query:\nSELECT * FROM Employee WHERE EmployeeID = 12345;\n\nHash Index:\n- hash(12345) % 100 = 45\n- Go directly to bucket 45\n- Cost: 1-2 I/Os (bucket + maybe overflow)\n\nB+ Tree Index:\n- Traverse tree: log_d(n) = 3 levels\n- Cost: 3 I/Os\n\nWinner: Hash (for exact match)\n\nRange Query:\nSELECT * FROM Employee WHERE EmployeeID BETWEEN 10000 AND 20000;\n\nHash Index:\n- No ordering information\n- Must scan all buckets\n- Cost: All buckets\n\nB+ Tree Index:\n- Find 10000, then sequential scan\n- Cost: log_d(n) + result size\n\nWinner: B+ Tree (for range)\n\nSQL Index Creation:\n\nCREATE INDEX idx_employee_dept ON Employee(DepartmentID);\nCREATE UNIQUE INDEX idx_employee_email ON Employee(Email);\nCREATE INDEX idx_order_date ON Orders(OrderDate);\n\n-- Composite index\nCREATE INDEX idx_employee_name ON Employee(LastName, FirstName);\n\n-- Covering index (includes non-key columns)\nCREATE INDEX idx_employee_salary ON Employee(DepartmentID) INCLUDE (Salary);\n\n-- Partial index (PostgreSQL)\nCREATE INDEX idx_active_orders ON Orders(OrderDate) WHERE Status='Active';",
          practical: "Best Practices:\n\n1. Index Strategy:\n   - Index foreign keys (join performance)\n   - Index WHERE clause columns\n   - Index ORDER BY columns\n   - Don't over-index (slows writes)\n   - Monitor index usage (drop unused)\n\n2. When to Use Each Index Type:\n   - B+ Tree: Default choice, handles all queries\n   - Hash: Exact match on high-cardinality columns\n   - Bitmap: Low-cardinality columns (gender, status)\n   - Full-text: Text search\n   - Spatial: Geographic data\n\n3. Clustering Decisions:\n   - Cluster on most frequent range query column\n   - Consider write vs read trade-off\n   - Only one clustered index per table\n\n4. Buffer Pool Tuning:\n   - PostgreSQL: shared_buffers (25% of RAM)\n   - MySQL: innodb_buffer_pool_size (70-80% of RAM)\n   - Monitor cache hit ratio (aim for >99%)\n\n5. Storage Hardware Considerations:\n   - SSD: Lower latency, random I/O less painful\n   - HDD: Optimize for sequential access\n   - Consider NVMe for extremely high throughput\n\nReal-World Scenarios:\n- E-commerce: Index product searches, cluster orders by date\n- Social media: Index user queries, hash indexes for user IDs\n- Analytics: Columnar storage, heavy compression\n- Banking: Cluster transactions by account, index timestamps",
          exam: "Important Questions:\n\n1. Compare heap, sorted, and hash file organizations. When to use each?\n2. Explain B+ Tree structure. Why preferred over B-Tree?\n3. Show step-by-step insertion/deletion in B+ Tree\n4. Calculate cost of query with and without index\n5. Explain buffer replacement policies: LRU, MRU, Clock\n6. What is a clustered index? Pros and cons?\n7. Compare hash index vs B+ Tree index\n8. Explain extendible hashing. How does it handle growth?\n9. What factors determine index selectivity?\n10. Design indexing strategy for given schema and query workload",
          takeaways: "Key Principles:\n\nâ€¢ Disk I/O is the primary bottleneck; minimize it with indexes and caching\nâ€¢ B+ Tree is the default index: handles all query types, keeps data sorted\nâ€¢ Hash indexes excel at exact matches but can't do ranges\nâ€¢ Clustered indexes physically order data, improving range queries\nâ€¢ Buffer pool caches hot pages, dramatically improving performance\nâ€¢ Over-indexing hurts write performance; balance reads vs writes\nâ€¢ Modern SSDs change trade-offs but don't eliminate I/O concerns"
        }
      },
      {
        id: "transaction-processing",
        title: "Transaction Processing",
        subtopics: ["Concurrency control", "ACID property", "Serializability", "Schedules", "Locking and timestamp based schedules", "Multi-version and optimistic concurrency control schemes", "Database recovery"],
        clos: ["CLO05"],
        cos: ["CO05"],
        content: {
          introduction: "Imagine transferring $500 from your savings to checking account. The bank's system must: (1) deduct $500 from savings, (2) add $500 to checking. What if the system crashes between steps? You'd lose $500! Transactions ensure that either both steps happen, or neither doesâ€”no in-between states.",
          concept: "A transaction is a logical unit of work that must be executed completely or not at all. Transactions guarantee database consistency even when facing failures (crashes, power outages) and concurrent access (multiple users).\n\nACID Properties:\n\nA - Atomicity: All or nothing\n  Either all operations succeed (commit) or none (abort/rollback)\n  No partial transactions\n\nC - Consistency: Valid state to valid state\n  Database constraints always satisfied\n  Business rules preserved\n\nI - Isolation: Concurrent transactions don't interfere\n  Each transaction sees consistent snapshot\n  Result same as if transactions ran serially\n\nD - Durability: Once committed, changes persist\n  Survives crashes, power failures\n  Written to stable storage\n\nTransaction States:\n\n1. Active: Initial state, executing\n2. Partially Committed: After final statement, before commit\n3. Committed: Successfully completed\n4. Failed: Cannot proceed (error, abort)\n5. Aborted: Rolled back, database restored\n\nAfter abort:\n- Restart transaction, OR\n- Kill transaction",
          technicalDepth: "CONCURRENCY CONTROL:\n\nProblem: Concurrent transactions can cause anomalies\n\n1. Lost Update:\n   T1: Read X=100\n   T2: Read X=100\n   T1: Write X=150 (added 50)\n   T2: Write X=120 (added 20)\n   Final: X=120 (T1's update lost!)\n\n2. Dirty Read (Reading Uncommitted Data):\n   T1: Write X=200\n   T2: Read X=200\n   T1: Rollback (X back to 100)\n   T2 saw data that never existed!\n\n3. Non-Repeatable Read:\n   T1: Read X=100\n   T2: Write X=200, Commit\n   T1: Read X=200\n   T1 sees different value in same transaction!\n\n4. Phantom Read:\n   T1: SELECT COUNT(*) WHERE age>20 (returns 10)\n   T2: INSERT person with age=25, Commit\n   T1: SELECT COUNT(*) WHERE age>20 (returns 11)\n   New row appeared!\n\nCONCURRENCY CONTROL PROTOCOLS:\n\n1. LOCK-BASED PROTOCOLS:\n\nLock Types:\n- Shared Lock (S): Read access, multiple allowed\n- Exclusive Lock (X): Write access, exclusive\n\nCompatibility Matrix:\n       S    X\n   S   Y    N\n   X   N    N\n\nTwo-Phase Locking (2PL):\n\nPhase 1 (Growing): Acquire locks, no releases\nPhase 2 (Shrinking): Release locks, no acquisitions\n\nGuarantees: Serializability (equivalent to some serial order)\n\nProblem: Can deadlock\n\nVariants:\n\na) Strict 2PL:\n   Hold all locks until commit/abort\n   Prevents dirty reads\n   Most commonly used\n\nb) Rigorous 2PL:\n   Hold all locks until commit/abort\n   AND release all at once\n\nDeadlock Example:\n   T1: Lock A\n   T2: Lock B\n   T1: Wait for B (held by T2)\n   T2: Wait for A (held by T1)\n   Deadlock!\n\nDeadlock Handling:\n\n1. Detection:\n   - Build wait-for graph\n   - Detect cycles\n   - Abort victim transaction\n\n2. Prevention:\n   - Wait-Die: Older waits, younger aborts\n   - Wound-Wait: Older preempts, younger waits\n   - Timeout: Abort if waiting too long\n\n3. Avoidance:\n   - Order resources\n   - All transactions acquire locks in same order\n\n2. TIMESTAMP-BASED PROTOCOLS:\n\nEach transaction assigned unique timestamp (start time)\nOrder transactions by timestamp\n\nRules:\n- TS(Ti) < TS(Tj) â‡’ Ti must finish before Tj starts\n\nFor each data item X, track:\n- W-timestamp(X): Last write timestamp\n- R-timestamp(X): Last read timestamp\n\nOn Ti reads X:\n  If TS(Ti) < W-timestamp(X): Abort Ti (too old)\n  Else: Execute, update R-timestamp(X)\n\nOn Ti writes X:\n  If TS(Ti) < R-timestamp(X): Abort Ti (too late)\n  If TS(Ti) < W-timestamp(X): Abort Ti (obsolete)\n  Else: Execute, update W-timestamp(X)\n\nAdvantage: No deadlocks\nDisadvantage: More aborts\n\n3. MULTIVERSION CONCURRENCY CONTROL (MVCC):\n\nIdea: Keep multiple versions of each data item\nReaders don't block writers, writers don't block readers\n\nEach write creates new version with timestamp\nReads see appropriate version based on transaction timestamp\n\nUsed by: PostgreSQL, Oracle, SQL Server\n\nExample:\n   T1 (TS=1): Read X (sees version X1)\n   T2 (TS=2): Write X (creates version X2)\n   T1: Read X (still sees X1, consistent snapshot)\n   T3 (TS=3): Read X (sees X2)\n\nRECOVERY:\n\nTypes of Failures:\n\n1. Transaction Failure:\n   - Logical error (division by zero)\n   - System error (deadlock)\n   Solution: Rollback transaction\n\n2. System Crash:\n   - Power failure, OS crash\n   - Memory contents lost\n   Solution: Recovery from log\n\n3. Disk Failure:\n   - Physical damage to disk\n   Solution: Restore from backup\n\nRecovery Techniques:\n\n1. LOG-BASED RECOVERY:\n\nWrite-Ahead Logging (WAL):\n   Rule: Log record must be written to disk BEFORE data\n\nLog Records:\n   <Ti start>\n   <Ti, X, old_value, new_value>\n   <Ti commit>\n   <Ti abort>\n\nRecovery Algorithm (ARIES):\n\nPhase 1: Analysis\n   - Determine which transactions to undo/redo\n   - Identify dirty pages\n\nPhase 2: Redo\n   - Repeat history (all changes, even aborted)\n   - Restore database to crash state\n\nPhase 3: Undo\n   - Rollback incomplete transactions\n   - Use log to reverse changes\n\n2. CHECKPOINTING:\n\nProblem: Log grows forever\nSolution: Periodic checkpoints\n\nCheckpoint Process:\n   1. Stop accepting new transactions\n   2. Write all dirty buffers to disk\n   3. Write <checkpoint> to log\n   4. Resume transactions\n\nRecovery:\n   - Start from last checkpoint (not beginning)\n   - Reduces recovery time\n\nISOLATION LEVELS (SQL Standard):\n\n1. Read Uncommitted:\n   - No isolation\n   - Allows: Dirty read, non-repeatable read, phantom\n   - Fastest, least safe\n\n2. Read Committed:\n   - Prevents dirty reads\n   - Allows: Non-repeatable read, phantom\n   - Default in many systems\n\n3. Repeatable Read:\n   - Prevents dirty reads, non-repeatable reads\n   - Allows: Phantom\n\n4. Serializable:\n   - Full isolation\n   - Prevents all anomalies\n   - Slowest, safest\n\nSQL Syntax:\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;",
          examples: "Example 1: Transaction in SQL\n\n-- Bank Transfer: Savings to Checking\nBEGIN TRANSACTION;\n\n-- Step 1: Deduct from savings\nUPDATE Account\nSET Balance = Balance - 500\nWHERE AccountID = 101 AND AccountType = 'Savings';\n\n-- Step 2: Add to checking\nUPDATE Account\nSET Balance = Balance + 500\nWHERE AccountID = 101 AND AccountType = 'Checking';\n\n-- Verify balance constraints\nIF (SELECT Balance FROM Account WHERE AccountID=101 AND AccountType='Savings') < 0\n  ROLLBACK;\nELSE\n  COMMIT;\nEND TRANSACTION;\n\nExample 2: Deadlock Scenario\n\nTime | Transaction T1              | Transaction T2\n-----|----------------------------|---------------------------\nt1   | BEGIN                      |\nt2   |                            | BEGIN\nt3   | LOCK A (Exclusive)         |\nt4   |                            | LOCK B (Exclusive)\nt5   | UPDATE A SET ...           |\nt6   |                            | UPDATE B SET ...\nt7   | LOCK B (Wait for T2)       |\nt8   |                            | LOCK A (Wait for T1)\nt9   | DEADLOCK DETECTED          |\nt10  | T2 ABORTED (Victim)        |\nt11  | LOCK B (Acquired)          |\nt12  | COMMIT                     |\n\nExample 3: Recovery Scenario\n\nLog Contents:\n<T1 start>\n<T1, X, 100, 150>\n<T1, Y, 200, 250>\n<T1 commit>\n<T2 start>\n<T2, X, 150, 200>\n<T2, Y, 250, 300>\n--- CRASH ---\n\nRecovery:\n1. Redo T1 (committed): X=150, Y=250\n2. Undo T2 (not committed): X=150 (revert T2's change), Y=250\n\nFinal State: X=150, Y=250\n\nExample 4: Isolation Levels in Action\n\n-- Session 1\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates Balance to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Reads 500 (DIRTY READ!)\nCOMMIT;\n\n-- With READ COMMITTED\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Still reads 1000\nCOMMIT;\n\n-- With SERIALIZABLE (PostgreSQL)\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRANSACTION;\nSELECT SUM(Balance) FROM Account; -- Reads sum\n-- (Session 2 inserts new account)\nSELECT SUM(Balance) FROM Account; -- Same sum (no phantom)\nCOMMIT;",
          practical: "Best Practices:\n\n1. Transaction Design:\n   - Keep transactions short\n   - Acquire locks in consistent order (prevent deadlock)\n   - Avoid user interaction within transaction\n   - Handle exceptions (rollback on error)\n\n2. Choosing Isolation Level:\n   - READ UNCOMMITTED: Never (except analytics)\n   - READ COMMITTED: Default for most applications\n   - REPEATABLE READ: Financial transactions, reports\n   - SERIALIZABLE: Critical data, when correctness > performance\n\n3. Performance Tuning:\n   - Monitor lock wait times\n   - Identify long-running transactions\n   - Use row-level locking (not table-level)\n   - Consider optimistic locking for low contention\n\n4. Deadlock Prevention:\n   - Access tables in same order\n   - Use timeouts\n   - Keep transactions short\n   - Retry aborted transactions\n\n5. Application-Level Considerations:\n   - Implement retry logic\n   - Use connection pooling\n   - Consider eventual consistency (NoSQL)\n   - Saga pattern for distributed transactions\n\nReal-World Usage:\n- Banking: SERIALIZABLE for transfers\n- E-commerce: READ COMMITTED for order processing\n- Social media: READ COMMITTED, eventual consistency for likes\n- Stock trading: SERIALIZABLE with strict locking\n- Analytics: READ UNCOMMITTED (stale data acceptable)",
          exam: "Key Questions:\n\n1. Explain ACID properties with examples\n2. Draw transaction state diagram\n3. What problems arise from concurrent transactions? Give examples.\n4. Explain Two-Phase Locking protocol. How does it ensure serializability?\n5. Compare timestamp-based vs lock-based concurrency control\n6. What is a deadlock? Explain detection and prevention strategies.\n7. Explain Write-Ahead Logging and its importance in recovery\n8. Describe ARIES recovery algorithm (Analysis, Redo, Undo)\n9. Compare SQL isolation levels. Which allows which anomalies?\n10. Given a schedule, determine if it's serializable\n11. Show recovery steps given transaction log and crash point\n12. Explain MVCC. Why do readers not block writers?",
          takeaways: "Core Concepts:\n\nâ€¢ ACID properties guarantee reliable transactions\nâ€¢ Atomicity: All or nothing (enforced by logging and recovery)\nâ€¢ Consistency: Maintain constraints (enforced by application + DBMS)\nâ€¢ Isolation: Concurrent transactions don't interfere (concurrency control)\nâ€¢ Durability: Committed changes survive crashes (write-ahead logging)\nâ€¢ Two-Phase Locking ensures serializability but can deadlock\nâ€¢ MVCC allows high concurrency by keeping multiple versions\nâ€¢ Recovery uses logs to redo committed and undo uncommitted transactions\nâ€¢ Isolation levels balance consistency with performance"
        }
      }
    ]
  },
  {
    id: "unit-5",
    title: "UNIT V: Security & Advanced Topics",
    topics: [
      {
        id: "database-security",
        title: "Database Security",
        subtopics: ["Authentication", "Authorization and access control", "DAC", "MAC and RBAC models", "Intrusion detection SQL injection"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "In 2023, data breaches exposed over 3 billion records globally. Equifax breach, Capital One breach, Marriott breachâ€”the list goes on. Database security isn't optional; it's essential. A single SQL injection or weak access control can expose millions of customer records, leading to financial losses, legal consequences, and destroyed trust.",
          concept: "Database security protects against unauthorized access, data breaches, and malicious attacks through multiple layers of defense.\n\nSecurity Objectives (CIA Triad):\n\n1. Confidentiality: Only authorized users access data\n   Prevent: Unauthorized disclosure, eavesdropping\n\n2. Integrity: Data accuracy and consistency\n   Prevent: Unauthorized modification, corruption\n\n3. Availability: Authorized users can access when needed\n   Prevent: DoS attacks, system failures\n\nSecurity Layers:\n\n1. Network Security: Firewalls, VPNs, encryption\n2. OS Security: User accounts, file permissions\n3. Database Security: Authentication, authorization, auditing\n4. Application Security: Input validation, secure coding\n\nKey Threats:\n- SQL Injection: Malicious SQL in user input\n- Unauthorized Access: Weak passwords, stolen credentials\n- Privilege Escalation: Gaining higher access than authorized\n- Insider Threats: Malicious employees\n- Data Leakage: Unencrypted data transmission\n- DoS/DDoS: Overwhelming system with requests",
          technicalDepth: "AUTHENTICATION:\n\nVerifying user identity\n\nMethods:\n\n1. Password-Based:\n   - Most common\n   - Store hashed passwords (SHA-256, bcrypt)\n   - Never store plaintext\n   - Salting prevents rainbow table attacks\n\n2. Multi-Factor Authentication (MFA):\n   - Something you know (password)\n   - Something you have (token, phone)\n   - Something you are (biometric)\n\n3. Certificate-Based:\n   - Digital certificates (X.509)\n   - Public key infrastructure (PKI)\n\n4. OAuth/SAML:\n   - Federated authentication\n   - Single Sign-On (SSO)\n\nAUTHORIZATION & ACCESS CONTROL:\n\nDetermining what authenticated users can do\n\n1. DISCRETIONARY ACCESS CONTROL (DAC):\n\nOwner controls access to resources\n\nSQL Commands:\nGRANT SELECT, INSERT ON Employees TO user1;\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\nREVOKE DELETE ON Orders FROM user2;\n\nGRANT Options:\n- SELECT: Read data\n- INSERT: Add new data\n- UPDATE: Modify existing data\n- DELETE: Remove data\n- EXECUTE: Run stored procedures\n\nWITH GRANT OPTION: User can grant permissions to others\nGRANT SELECT ON Employees TO user1 WITH GRANT OPTION;\n\nProblems with DAC:\n- Permissions can spread uncontrollably\n- No central policy enforcement\n- Vulnerable to Trojan horse attacks\n\n2. MANDATORY ACCESS CONTROL (MAC):\n\nSystem enforces access policy based on security labels\n\nConcepts:\n- Security Clearance: User's level (Top Secret, Secret, Unclassified)\n- Security Classification: Data's level\n- Bell-LaPadula Model: \n  - No Read Up: Can't read higher classification\n  - No Write Down: Can't write to lower classification\n  - Prevents information leakage\n\nExample:\nUser with SECRET clearance:\n- Can read: UNCLASSIFIED, SECRET\n- Cannot read: TOP SECRET\n- Can write: SECRET, TOP SECRET\n- Cannot write: UNCLASSIFIED (prevents leaking)\n\nUsed in: Military, government systems\n\n3. ROLE-BASED ACCESS CONTROL (RBAC):\n\nPermissions assigned to roles, users assigned to roles\n\nAdvantages:\n- Easier management (modify role, not individual users)\n- Principle of least privilege\n- Separation of duties\n\nSQL Implementation:\n\nCREATE ROLE hr_manager;\nGRANT SELECT, UPDATE ON Employees TO hr_manager;\nGRANT SELECT ON Salaries TO hr_manager;\n\nCREATE ROLE accountant;\nGRANT SELECT ON Salaries TO accountant;\nGRANT INSERT, UPDATE ON Expenses TO accountant;\n\nGRANT hr_manager TO alice;\nGRANT accountant TO bob;\n\nRole Hierarchies:\nCREATE ROLE manager;\nGRANT hr_manager TO manager; -- Manager inherits HR permissions\n\nSQL INJECTION:\n\nAttack Mechanism:\nUser input interpreted as SQL code\n\nVulnerable Code (Python):\nusername = request.form['username']\npassword = request.form['password']\nquery = f\"SELECT * FROM Users WHERE username='{username}' AND password='{password}'\"\nexecute(query)\n\nAttack Input:\nusername: admin' --\npassword: anything\n\nResulting Query:\nSELECT * FROM Users WHERE username='admin' -- ' AND password='anything'\n(-- comments out rest, bypasses password check)\n\nWorse Attack (Union-based):\nusername: ' UNION SELECT * FROM CreditCards --\n\nResulting Query:\nSELECT * FROM Users WHERE username='' UNION SELECT * FROM CreditCards -- '\n(Returns credit card data!)\n\nPrevention:\n\n1. Parameterized Queries (Prepared Statements):\n\nSecure Code (Python):\nquery = \"SELECT * FROM Users WHERE username=? AND password=?\"\nexecute(query, (username, password))\n\nDatabase treats ? as data, not code\n\n2. Stored Procedures:\nCREATE PROCEDURE AuthenticateUser(@username NVARCHAR(50), @password NVARCHAR(50))\nAS\nBEGIN\n  SELECT * FROM Users WHERE username=@username AND password=@password;\nEND;\n\nCALL AuthenticateUser('admin', 'pass123');\n\n3. Input Validation:\n- Whitelist allowed characters\n- Reject suspicious input ('; --, UNION, etc.)\n- Escape special characters\n\n4. Least Privilege:\n- Application database account has minimal permissions\n- No DROP, CREATE permissions\n\n5. Web Application Firewall (WAF):\n- Detects and blocks SQL injection patterns\n\nENCRYPTION:\n\n1. Data at Rest:\n   - Encrypt database files\n   - Transparent Data Encryption (TDE)\n   - Column-level encryption for sensitive fields\n\n2. Data in Transit:\n   - SSL/TLS for connections\n   - HTTPS for web applications\n\n3. Key Management:\n   - Secure key storage (HSM, Key Vault)\n   - Key rotation policies\n\nAUDITING:\n\nTrack who accessed what, when\n\nAudit Log Contents:\n- User ID\n- Timestamp\n- Action (SELECT, INSERT, UPDATE, DELETE)\n- Table/row accessed\n- Success/failure\n- Source IP\n\nSQL Server Audit:\nCREATE SERVER AUDIT CompanyAudit\nTO FILE (FILEPATH = 'C:\\Audits\\')\nWITH (ON_FAILURE = CONTINUE);\n\nCREATE DATABASE AUDIT SPECIFICATION SalaryAudit\nFOR SERVER AUDIT CompanyAudit\nADD (SELECT, UPDATE ON Salaries BY public);\n\nBenefits:\n- Detect unauthorized access\n- Forensics after breach\n- Compliance (GDPR, HIPAA, SOX)",
          examples: "Example 1: SQL Injection Attack & Prevention\n\nVulnerable Login (PHP):\n$username = $_POST['username'];\n$password = $_POST['password'];\n$query = \"SELECT * FROM users WHERE username='$username' AND password='$password'\";\n$result = mysqli_query($conn, $query);\n\nAttack:\nUsername: admin' OR '1'='1\nPassword: anything\n\nQuery becomes:\nSELECT * FROM users WHERE username='admin' OR '1'='1' AND password='anything'\n(Always true, logs in as admin!)\n\nSecure Version (Prepared Statement):\n$stmt = $conn->prepare(\"SELECT * FROM users WHERE username=? AND password=?\");\n$stmt->bind_param(\"ss\", $username, $password);\n$stmt->execute();\n$result = $stmt->get_result();\n\nExample 2: RBAC Implementation\n\n-- Create roles\nCREATE ROLE sales_rep;\nCREATE ROLE sales_manager;\nCREATE ROLE admin;\n\n-- Grant permissions to roles\nGRANT SELECT ON Customers TO sales_rep;\nGRANT SELECT, INSERT ON Orders TO sales_rep;\n\nGRANT SELECT ON Customers TO sales_manager;\nGRANT SELECT, INSERT, UPDATE ON Orders TO sales_manager;\nGRANT SELECT ON SalesReport TO sales_manager;\n\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\n\n-- Assign users to roles\nGRANT sales_rep TO john, mary;\nGRANT sales_manager TO alice;\nGRANT admin TO bob;\n\n-- Role hierarchy (manager inherits rep permissions)\nGRANT sales_rep TO sales_manager;\n\nExample 3: Encryption\n\n-- Column-level encryption (SQL Server)\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'StrongPassword123!';\n\nCREATE CERTIFICATE SSNCert\nWITH SUBJECT = 'SSN Encryption Certificate';\n\nCREATE SYMMETRIC KEY SSNKey\nWITH ALGORITHM = AES_256\nENCRYPTION BY CERTIFICATE SSNCert;\n\n-- Encrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nINSERT INTO Employees (Name, SSN_Encrypted)\nVALUES ('John Doe', EncryptByKey(Key_GUID('SSNKey'), '123-45-6789'));\n\nCLOSE SYMMETRIC KEY SSNKey;\n\n-- Decrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nSELECT Name, CONVERT(VARCHAR, DecryptByKey(SSN_Encrypted)) AS SSN\nFROM Employees;\n\nCLOSE SYMMETRIC KEY SSNKey;\n\nExample 4: Auditing\n\n-- Enable auditing for sensitive table (PostgreSQL)\nCREATE TABLE audit_log (\n    audit_id SERIAL PRIMARY KEY,\n    table_name VARCHAR(50),\n    action VARCHAR(10),\n    user_name VARCHAR(50),\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    old_data JSON,\n    new_data JSON\n);\n\n-- Trigger to log changes\nCREATE OR REPLACE FUNCTION log_salary_changes()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'UPDATE' THEN\n        INSERT INTO audit_log (table_name, action, user_name, old_data, new_data)\n        VALUES ('Salaries', 'UPDATE', current_user, \n                row_to_json(OLD), row_to_json(NEW));\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER salary_audit\nAFTER UPDATE ON Salaries\nFOR EACH ROW EXECUTE FUNCTION log_salary_changes();",
          practical: "Best Practices:\n\n1. Defense in Depth:\n   - Multiple security layers\n   - Firewall + Authentication + Authorization + Encryption + Auditing\n   - If one fails, others still protect\n\n2. Principle of Least Privilege:\n   - Grant minimum necessary permissions\n   - Application account: Only needed tables\n   - Read-only users: No write permissions\n\n3. Input Validation:\n   - Validate all user input\n   - Use parameterized queries\n   - Whitelist, not blacklist\n\n4. Regular Security Audits:\n   - Review user permissions\n   - Check audit logs for anomalies\n   - Penetration testing\n\n5. Encryption:\n   - Encrypt sensitive data at rest\n   - Use SSL/TLS for connections\n   - Secure key management\n\n6. Patch Management:\n   - Keep database software updated\n   - Apply security patches promptly\n\n7. Monitoring & Alerting:\n   - Monitor failed login attempts\n   - Alert on unusual access patterns\n   - Real-time intrusion detection\n\nCompliance Standards:\n- GDPR: EU data protection\n- HIPAA: Healthcare data (US)\n- PCI DSS: Credit card data\n- SOX: Financial reporting\n\nReal-World Scenarios:\n- Healthcare: Encrypt patient records, audit all access\n- Banking: MFA, transaction auditing, encryption\n- E-commerce: PCI DSS compliance, secure payment data\n- Government: MAC for classified data",
          exam: "Important Questions:\n\n1. Explain the CIA triad in database security\n2. Compare DAC, MAC, and RBAC with examples\n3. What is SQL injection? How to prevent it?\n4. Show SQL injection attack and secure code\n5. Explain role-based access control with SQL examples\n6. What is the difference between authentication and authorization?\n7. Why use parameterized queries instead of string concatenation?\n8. Explain encryption at rest vs encryption in transit\n9. What is database auditing? Why is it important?\n10. Explain multi-factor authentication\n11. What is the principle of least privilege?\n12. How does GRANT and REVOKE work in SQL?",
          takeaways: "Key Principles:\n\nâ€¢ Security requires multiple layers: authentication, authorization, encryption, auditing\nâ€¢ SQL injection is preventable: use parameterized queries, never concatenate SQL\nâ€¢ RBAC simplifies permission management: assign permissions to roles, not individuals\nâ€¢ Principle of least privilege: grant minimum necessary permissions\nâ€¢ Encryption protects data at rest and in transit\nâ€¢ Auditing provides accountability and forensics\nâ€¢ Defense in depth: multiple security layers provide redundancy\nâ€¢ Security is ongoing: monitor, patch, audit regularly"
        }
      },
      {
        id: "advanced-topics",
        title: "Advanced Topics",
        subtopics: ["Object oriented and object relational databases", "Logical warehousing and data mining"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "Relational databases dominated for decades, but new challenges emerged: How to store complex objects? How to analyze petabytes of data? How to handle unstructured data? Advanced database technologiesâ€”object-oriented databases, data warehousing, NoSQL, and distributed systemsâ€”evolved to address these needs.",
          concept: "Advanced database topics extend beyond traditional relational databases to handle complex data types, massive-scale analytics, and modern application requirements.\n\nKey Motivations:\n\n1. Complex Data: Multimedia, CAD, scientific simulations\n2. Big Data: Petabytes of data, distributed processing\n3. Performance: Real-time analytics, low-latency access\n4. Scalability: Horizontal scaling, cloud-native\n5. Flexibility: Schema evolution, heterogeneous data",
          technicalDepth: "OBJECT-ORIENTED DATABASES (OODBMS):\n\nMotivation: Impedance mismatch between OOP languages and relational databases\n\nRelational Approach:\nclass Employee {\n    int id;\n    String name;\n    Address address; // Complex object\n}\n\n// Must decompose into multiple tables\nEmployees(id, name, address_id)\nAddresses(address_id, street, city, zip)\n\n// Retrieve requires joins, complex mapping\n\nOODBMS Approach:\nStore objects directly, no decomposition\nQuery using object-oriented syntax\n\nFeatures:\n\n1. Complex Objects:\n   - Nested objects\n   - Collections (lists, sets)\n   - References between objects\n\n2. Object Identity (OID):\n   - Each object has unique identifier\n   - Independent of attribute values\n   - Persists across sessions\n\n3. Encapsulation:\n   - Methods stored with data\n   - Behavior + data together\n\n4. Inheritance:\n   - Class hierarchies\n   - Subclasses inherit from superclasses\n\n5. Polymorphism:\n   - Method overriding\n   - Dynamic binding\n\nOODBMS Examples:\n- db4o (Java, .NET)\n- ObjectDB (Java)\n- Versant\n\nObject-Relational Databases (ORDBMS):\n\nHybrid: Relational model + OO features\n\nPostgreSQL Example:\n-- User-defined types\nCREATE TYPE Address AS (\n    street VARCHAR(100),\n    city VARCHAR(50),\n    zip VARCHAR(10)\n);\n\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    address Address  -- Complex type\n);\n\nINSERT INTO Employee VALUES (1, 'John', ROW('123 Main St', 'NYC', '10001'));\n\nSELECT name, (address).city FROM Employee;\n\nAdvantages:\n- Natural mapping from OOP\n- No joins for complex objects\n- Better performance for complex data\n\nDisadvantages:\n- Less mature than RDBMS\n- Smaller ecosystem\n- No standard query language (unlike SQL)\n\nDATA WAREHOUSING:\n\nDefinition: Subject-oriented, integrated, time-variant, non-volatile collection for decision support\n\nCharacteristics:\n\n1. Subject-Oriented: Organized by business area (Sales, Finance)\n2. Integrated: Data from multiple sources\n3. Time-Variant: Historical data, snapshots\n4. Non-Volatile: Read-only, no updates\n\nArchitecture:\n\nSource Systems (OLTP)\n    â†“\nETL (Extract, Transform, Load)\n    â†“\nData Warehouse\n    â†“\nData Marts (department-specific)\n    â†“\nOLAP / BI Tools\n\nOLTP vs OLAP:\n\nOLTP (Online Transaction Processing):\n- Current data\n- Detailed records\n- Read/Write\n- Normalized (3NF)\n- Fast transactions\n- Example: Bank account balance\n\nOLAP (Online Analytical Processing):\n- Historical data\n- Aggregated summaries\n- Read-mostly\n- Denormalized (star schema)\n- Complex queries\n- Example: Quarterly sales analysis\n\nDimensional Modeling:\n\nStar Schema:\n\n        Dimension: Time\n             |\n             |\nDimension -- Fact Table -- Dimension\n (Product)   (Sales)      (Customer)\n             |\n             |\n        Dimension: Store\n\nFact Table: Measures (sales amount, quantity)\nDimension Tables: Context (who, what, when, where)\n\nSnowflake Schema:\nNormalized dimensions (dimension tables have sub-dimensions)\n\nOLAP Operations:\n\n1. Roll-Up: Aggregate to higher level\n   Daily sales â†’ Monthly sales\n\n2. Drill-Down: Detail to lower level\n   Yearly sales â†’ Quarterly sales\n\n3. Slice: Fix one dimension\n   Sales in Q1 2024\n\n4. Dice: Fix multiple dimensions\n   Sales in Q1 2024, Region=West, Product=Laptop\n\n5. Pivot: Rotate view\n   Rows â†” Columns\n\nDATA MINING:\n\nDefinition: Discovering patterns, correlations, trends in large datasets\n\nKnowledge Discovery Process (KDD):\n\n1. Data Cleaning: Remove noise, outliers\n2. Data Integration: Combine sources\n3. Data Selection: Choose relevant data\n4. Data Transformation: Normalize, aggregate\n5. Data Mining: Apply algorithms\n6. Pattern Evaluation: Identify interesting patterns\n7. Knowledge Presentation: Visualize, report\n\nData Mining Techniques:\n\n1. Association Rules:\n   Market Basket Analysis\n   {Bread, Butter} â‡’ {Milk} (support=30%, confidence=80%)\n   Support: % transactions containing itemset\n   Confidence: % transactions with antecedent that also have consequent\n\n2. Classification:\n   Assign items to predefined categories\n   Decision Trees, Neural Networks, SVM\n   Example: Spam vs Not Spam email\n\n3. Clustering:\n   Group similar items (no predefined categories)\n   K-Means, Hierarchical Clustering\n   Example: Customer segmentation\n\n4. Regression:\n   Predict continuous values\n   Linear Regression, Polynomial Regression\n   Example: Predict house price\n\n5. Anomaly Detection:\n   Identify outliers\n   Example: Fraud detection\n\n6. Sequential Patterns:\n   Find patterns over time\n   Example: Web clickstream analysis\n\nDISTRIBUTED DATABASES:\n\nData stored across multiple locations\n\nTypes:\n\n1. Homogeneous: Same DBMS at all sites\n2. Heterogeneous: Different DBMS at different sites\n\nFragmentation:\n\n1. Horizontal: Split rows\n   West_Customers (CA, WA, OR)\n   East_Customers (NY, MA, PA)\n\n2. Vertical: Split columns\n   Employee_Basic (id, name, dept)\n   Employee_Payroll (id, salary, bonus)\n\n3. Hybrid: Both\n\nReplication:\n- Full: Complete copy at each site\n- Partial: Selected fragments at sites\n\nAdvantages:\n- Improved performance (data locality)\n- Increased availability (redundancy)\n- Scalability\n\nChallenges:\n- Distributed query processing\n- Distributed transactions (2PC)\n- Consistency vs Availability (CAP theorem)\n\nNOSQL DATABASES:\n\nMotivation: Scalability, flexibility, performance for web-scale applications\n\nTypes:\n\n1. Key-Value: Redis, DynamoDB\n   Simple: key â†’ value\n   Fast lookups\n   Use case: Session storage, caching\n\n2. Document: MongoDB, CouchDB\n   Store JSON/BSON documents\n   Schema-less\n   Use case: Content management, catalogs\n\n3. Column-Family: Cassandra, HBase\n   Wide columns, billions of columns\n   Distributed, high write throughput\n   Use case: Time-series, logs\n\n4. Graph: Neo4j, Amazon Neptune\n   Nodes and edges\n   Relationships are first-class\n   Use case: Social networks, recommendations\n\nCAP Theorem:\nCan have at most 2 of 3:\n- Consistency: All nodes see same data\n- Availability: Every request gets response\n- Partition Tolerance: Works despite network splits\n\nRelational: CA (sacrifice partition tolerance)\nNoSQL: Often AP or CP",
          examples: "Example 1: OODBMS (db4o - Java)\n\n// Define classes\nclass Employee {\n    String name;\n    Address address;\n    List<Project> projects;\n}\n\nclass Address {\n    String street;\n    String city;\n}\n\n// Store object directly\nEmployee emp = new Employee();\nemp.name = \"John\";\nemp.address = new Address(\"123 Main\", \"NYC\");\ndb.store(emp); // Stores entire object graph\n\n// Query\nList<Employee> result = db.query(new Predicate<Employee>() {\n    public boolean match(Employee emp) {\n        return emp.address.city.equals(\"NYC\");\n    }\n});\n\nExample 2: Data Warehouse Star Schema (SQL)\n\nCREATE TABLE FactSales (\n    sale_id INT PRIMARY KEY,\n    date_id INT,\n    product_id INT,\n    customer_id INT,\n    store_id INT,\n    quantity INT,\n    amount DECIMAL(10,2),\n    FOREIGN KEY (date_id) REFERENCES DimDate(date_id),\n    FOREIGN KEY (product_id) REFERENCES DimProduct(product_id),\n    FOREIGN KEY (customer_id) REFERENCES DimCustomer(customer_id),\n    FOREIGN KEY (store_id) REFERENCES DimStore(store_id)\n);\n\nCREATE TABLE DimDate (\n    date_id INT PRIMARY KEY,\n    date DATE,\n    day INT,\n    month INT,\n    quarter INT,\n    year INT\n);\n\nCREATE TABLE DimProduct (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    category VARCHAR(50),\n    brand VARCHAR(50)\n);\n\n-- Analytical Query: Quarterly sales by product category\nSELECT d.quarter, p.category, SUM(f.amount) AS total_sales\nFROM FactSales f\nJOIN DimDate d ON f.date_id = d.date_id\nJOIN DimProduct p ON f.product_id = p.product_id\nWHERE d.year = 2024\nGROUP BY d.quarter, p.category\nORDER BY d.quarter, total_sales DESC;\n\nExample 3: NoSQL (MongoDB)\n\n// Document database - store complex objects\ndb.employees.insertOne({\n    _id: 1,\n    name: \"John Doe\",\n    address: {\n        street: \"123 Main St\",\n        city: \"NYC\",\n        zip: \"10001\"\n    },\n    projects: [\n        { name: \"Project A\", role: \"Lead\" },\n        { name: \"Project B\", role: \"Developer\" }\n    ],\n    skills: [\"Java\", \"Python\", \"SQL\"]\n});\n\n// Query nested fields\ndb.employees.find({ \"address.city\": \"NYC\" });\n\n// Query arrays\ndb.employees.find({ skills: \"Python\" });\n\nExample 4: Data Mining - Association Rules (Python)\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport pandas as pd\n\n# Transaction data\ntransactions = [\n    ['Bread', 'Milk', 'Butter'],\n    ['Bread', 'Butter'],\n    ['Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk']\n]\n\n# Find frequent itemsets\nfrequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\nprint(rules[['antecedents', 'consequents', 'support', 'confidence']])\n\n# Output: {Bread, Butter} => {Milk} (confidence=0.8)",
          practical: "When to Use Each Technology:\n\n1. Relational Databases (PostgreSQL, MySQL):\n   - Structured data\n   - ACID transactions required\n   - Complex queries, joins\n   - Examples: Banking, e-commerce orders\n\n2. OODBMS / ORDBMS:\n   - Complex nested objects\n   - Multimedia, CAD, scientific data\n   - Tight OOP integration\n   - Examples: GIS, engineering simulations\n\n3. Data Warehouses:\n   - Historical analysis\n   - Business intelligence\n   - Reporting, dashboards\n   - Examples: Sales analytics, financial reporting\n\n4. NoSQL:\n   Document (MongoDB): Flexible schema, content management\n   Key-Value (Redis): Caching, session storage\n   Column-Family (Cassandra): Time-series, logs, high write throughput\n   Graph (Neo4j): Social networks, recommendations, fraud detection\n\n5. Distributed Databases:\n   - Global applications\n   - High availability requirements\n   - Scale beyond single server\n   - Examples: Multi-region applications\n\nModern Architecture Trends:\n- Polyglot Persistence: Use right database for each use case\n- Lambda Architecture: Batch + Stream processing\n- Data Lakes: Store raw data, process on-demand\n- Cloud Data Warehouses: Snowflake, BigQuery, Redshift",
          exam: "Key Questions:\n\n1. Compare relational databases with object-oriented databases\n2. What is impedance mismatch? How does OODBMS solve it?\n3. Explain star schema and snowflake schema\n4. What is the difference between OLTP and OLAP?\n5. Describe the ETL process in data warehousing\n6. Explain OLAP operations: roll-up, drill-down, slice, dice, pivot\n7. What is data mining? List and explain 3 techniques\n8. Explain association rule mining with example\n9. What is the CAP theorem? How does it relate to NoSQL?\n10. Compare document, key-value, column-family, and graph databases\n11. What is horizontal vs vertical fragmentation in distributed databases?\n12. When would you choose NoSQL over relational database?",
          takeaways: "Core Concepts:\n\nâ€¢ OODBMS stores objects directly, eliminating impedance mismatch\nâ€¢ Data warehouses support analytics with denormalized star/snowflake schemas\nâ€¢ OLTP focuses on transactions, OLAP on analysis\nâ€¢ Data mining discovers patterns: association rules, classification, clustering\nâ€¢ NoSQL provides scalability and flexibility for specific use cases\nâ€¢ CAP theorem: Can't have consistency, availability, and partition tolerance together\nâ€¢ Distributed databases provide scalability through fragmentation and replication\nâ€¢ Modern applications use polyglot persistence: right database for each use case"
        }
      }
    ]
  }
];
